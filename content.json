{"posts":[{"title":"Batch Normalize - 手写","text":"2022-05-14 打卡； Referencehttps://zhuanlan.zhihu.com/p/34879333","link":"/daka/posts/322c39cd/"},{"title":"Layer Normalize - 手写","text":"2022-05-18 打卡；","link":"/daka/posts/e338883e/"},{"title":"打卡规则","text":"","link":"/daka/posts/2c85040f/"},{"title":"Batch Normalize - markdown","text":"2022-05-15 打卡； 1、BN具体的操作步骤其操作步骤分为两部分： 每个 mini-batch 内的特征通过减去 $\\mu$ 除以 $\\sigma$ 的方式归一化到标准正态分布； 对经过上一步骤的输出再过一个线性变换； 以上是文字形式的说明，下面是公式形式。 输入： 一个 mini-batch 内的数据为 $ \\{x_1, x_2, …, x_m \\}$，其中 $m$ 是 mini-batch-size； 记 $x_i^{(j)}$ 为该 mini-batch 中第 $i$ 条数据的第 $j$ 个特征； $\\gamma$ 和 $\\beta$ 为可学习的参数； 输出： $y_i^{(j)}$ 为该 mini-batch 中第 $i$ 条数据的第 $j$ 个特征经过BN后的结果。 公式： 该 mini-batch 中第j个特征的均值： $$\\mu_{B}^{(j)} = \\frac{1}{m} \\sum_{i=1}^{m} x_{i}^{(j)}$$ 该 mini-batch 中第j个特征的方差： $$ \\sigma_{B}^{(j)2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_i^{(j)} - \\mu_B^{(j)})^2$$ 减去均值，除上标准化，$\\epsilon$ 用于避免除数为0： $$\\hat{x}_i^{(j)} = \\frac{x_i^{(j)} - \\mu_B^{(j)}}{\\sqrt{\\sigma_B^{(j)2} + \\epsilon}} $$ 经过BN后的最终输出结果： $$y_i^{(j)} = \\gamma \\hat{x_i^{(j)}} + \\beta $$ 2、BN是为了解决什么问题2.1 ICS问题（Internal Covariate Shift）在深度模型中，随着训练的进行，网络中的参数随着梯度下降在不断更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活，这些微弱变化随着网络层数的加深而被加大；另一方面，参数的变化导致每一层的输入分布会发生变化，进而上层的网络需要不停的去适应这些分布的变化，使的模型训练变得困难。这一现象就是ICS问题。 2.2 ICS会导致的问题 由于下层网络的输出数据分布在不断变化，上层网络就需要不停调整来适应这个变化，这会导致网络学习速度的降低； 随着模型训练的进行，模型的参数 $W$ 会变大，则每层的输出 $ \\text{Output} = W * \\text{Input} + b $ 也会变大，当使用饱和激活函数时，就容易陷入梯度饱和区，此时的梯度就会很小甚至于接近0. 3、在BN出现前，是如何解决上述问题的3.1 白化操作ICS产生的原因是由于参数的更新带来的网络中每一层输入值分布的改变，并且随着网络的加深而越严重。缓解ICS最直观的想法就是：固定每层网络输入值的分布来缓解ICS问题。 白化操作：经过变化操作之后会具有如下两个性质： 经过白化后，其特征之间的相关性较低； 经过白化后，所有特征具有相同的方差； 在BN出现之前，主要是利用白化操作来缓解ICS问题：主要是PCA白化和ZCA白化。 3.2 白化操作的缺点 计算成本太高； 白化操作实际上是改变了网络每一层的分布，所以其也改变了网络的表达能力； 4、BN的作用与优势 BN将网络每一层输入数据的分布固定在一定的范围内，使上层网络不需频繁的去适应底层网络，加速了模型的学习速度； BN能够使每层网络的输入落在饱和激活函数的非饱和区，缓解梯度消失问题； BN具有一定的正则化效果，由于每个 mini-batch 的均值与方差不完全相同，相当于增加了随机噪声，使其具有一定的正则化效果； 由于BN操作中可学习参数 $\\gamma$ 和 $\\beta$ 的存在，BN缓解了上述所说的白化操作的第2个缺点； 经过BN操作之后，模型权重的缩放（比如 $W =&gt; aW$ ）会被“抹去”，这使模型对网络中的参数不再过分敏感，训练时更加稳定。 Referencehttps://zhuanlan.zhihu.com/p/34879333 https://www.zhihu.com/question/395811291 https://www.zhihu.com/question/487766088","link":"/daka/posts/659613b9/"},{"title":"KL散度 - 001","text":"2022-06-13 打卡；","link":"/daka/posts/925aad3c/"},{"title":"Layer Normalize - markdown","text":"2022-05-22 打卡； 1、LN的具体操作步骤其操作步骤可分为三部分： 求每条数据各特征之间的均值和标准差； 每条数据的每个特征减去各自数据的均值，除上各自数据的标准差； 对经过上一步骤的输出再经过一个线性变换； 以上是文字形式的说明，以下是公式形式。 输入： 一个 mini-batch 的数据在某层网络的输出为 $\\{\\alpha_1, \\alpha_2, …, \\alpha_m\\}$，其中 $m$ 为batch size； 记 $\\alpha_i^{(j)}$ 为该mini-batch中第 $i$ 条数据的第 $j$ 个特征； $T$ 为每条数据的特征数，每条数据的特征数不一定相同； $g$ 和 $b$ 为可学习参数； 按照上述定义，某层网络的输出的shape为 $[m, T]$，$m$ 为batch-size，$T$ 为每条数据的特征数量； 输出： $y_i^{(j)}$ 为该 mini-batch 中第 $i$ 条数据第 $j$ 个特征经过LN之后的输出； 公式： 第 $i$ 条数据各特征的均值： $$\\mu_i = \\frac{1}{T} \\sum_{j=1}^{T} \\alpha_i^{(j)} $$ 第 $i$ 条数据各特征的方差： $$\\sigma_i^2 = \\frac{1}{T} \\sum_{j=1}^{T} (\\alpha_i^{(j)} - \\mu_i)^2$$ 减去均值，除上标准化，$\\epsilon$ 用于避免除数为0： $$\\hat{\\alpha_i^{(j)}} = \\frac{\\alpha_i^{(j)} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}} $$ 第 $i$ 条数据第 $j$ 个特征经过LN后的结果： $$y_i^{(j)} = g \\hat{\\alpha_i^{(j)}} + b $$ 2、LN 为了解决什么问题 深度模型训练时所需要的计算资源非常大，想要减少训练所需时间的一个方法是：normalize the activities of neurons 增加训练过程的稳定性； 3、LN 出现之前是如何解决上述问题的LN 出现之前通过 BN 解决上述问题； BN的优点： 可以解决 “convariate shift” 问题，缩短了模型训练所需的时间； 能够使饱和激活函数的输入落在非饱和区，增加了训练的稳定性； BN的缺点： 当 batch size 特别小时，表现不好； 当每条数据的长度不一致时，比如文本数据，效果不好； 在 RNN 网络中，表现不好； 4、LN 的优势Normalization 的作用：降低了对参数初始化的需求，允许使用更大的学习率，有一定的正则化作用可抗过拟合，使训练更加稳定。 假设某一层输出的中间结果为 $[m, T]$，$m$ 为batch-size，$T$ 为每条数据的特征数量，那么： BN 是对 $m$ 这个维度做归一化； LN 是对 $T$ 这个维度做归一化； 优势（以下都有待考证）： 在 RNN 网络中，表现较好； 在 batch size 较小的网络中，表现较好； LN 抹杀了不同样本间的大小关系，保留了同一个样本内部的特征之间的大小关系，这对于时间序列任务或NLP任务来说非常重要； 5、LN效果测试代码123456789101112131415161718192021222324import torchimport torch.nn as nn# NLP例子，一般在NLP任务中，其维度为[batch_size, seq_len, hidden_dim]，LayerNorm操作仅对最后一个维度做操作batch, sentence_length, hidden_dim = 20, 5, 10embedding = torch.randn(batch, sentence_length, hidden_dim)print(&quot;LayerNorm前, 均值: &quot;)mean_result = embedding.mean((-1)) # 计算维度 hidden_dim 的均值print([f&quot;%.2f&quot; % float(y) for x in mean_result.detach().numpy().tolist() for y in x][:20], &quot;...&quot;)print(&quot;LayerNorm前, 方差: &quot;)var_result = embedding.var((-1)) # 计算维度 hidden_dim 的方差print([f&quot;%.2f&quot; % float(y) for x in var_result.detach().numpy().tolist() for y in x][:20], &quot;...&quot;)# 该LayerNorm层的input的维度为[*, hidden_dim]，其仅对初始化时给定的hidden_dim这个维度做归一化layer_norm = nn.LayerNorm(hidden_dim)embedding = layer_norm(embedding)print(&quot;LayerNorm后, 均值: &quot;)mean_result = embedding.mean((-1)) # 计算维度 hidden_dim 的均值print([f&quot;%.2f&quot; % float(y) for x in mean_result.detach().numpy().tolist() for y in x][:20], &quot;...&quot;)print(&quot;LayerNorm后, 方差: &quot;)var_result = embedding.var((-1)) # 计算维度 hidden_dim 的方差print([f&quot;%.2f&quot; % float(y) for x in var_result.detach().numpy().tolist() for y in x][:20], &quot;...&quot;) 输出结果： 12345678LayerNorm前, 均值: ['0.23', '0.23', '0.18', '-0.06', '-0.45', '-0.24', '0.34', '0.23', '-0.47', '-0.44', '0.12', '-0.26', '-0.37', '0.33', '-0.50', '0.11', '0.14', '0.37', '-0.12', '0.31'] ...LayerNorm前, 方差: ['1.34', '0.51', '1.16', '0.90', '0.17', '0.50', '0.56', '0.61', '0.70', '1.06', '0.85', '1.26', '1.34', '1.45', '1.52', '0.75', '0.63', '1.37', '1.34', '1.51'] ...LayerNorm后, 均值: ['0.00', '-0.00', '-0.00', '0.00', '0.00', '-0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '-0.00', '0.00', '0.00', '-0.00', '-0.00', '-0.00', '0.00'] ...LayerNorm后, 方差: ['1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11', '1.11'] ... 可以看出，经过归一化之后，其均值为0，方差为1.11（这里为什么是1.11，而不是1，还没搞清楚）；","link":"/daka/posts/e661f3c9/"},{"title":"KL散度 - markdown","text":"2022-06-14 打卡； 1、定义KL散度是一种衡量两个概率分布 P 和 Q 的差异的方法； 对于连续随机变量的概率分布来说，KL散度公式为： $$D_{KL}(p||q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx$$ 对于离散随机变量的概率分布来说，KL散度公式为： $$D_{KL}(p||q) = \\sum_x p(x) \\log \\frac{p(x)}{q(x)}$$ 机器学习和深度学习中使用的都是离散随机变量的概率分布，下面将仅讨论离散情况下的KL散度。 2、在机器学习中KL散度的作用机器学习的目标就是：希望模型学到的分布 $p_{model}$ 与该任务的真实分布 $P_{real}$ 一致。 问题在于该任务的真实分布 $P_{real}$ 是无法获取到的，能够获取到的是训练集的分布 $P_{train}$，我们一般认为训练数据是从总体中独立同分布采样出来的，基于该条件下，就可以认为训练集的分布 $P_{train}$ 与真实分布 $P_{real}$ 是一致的。这样机器学习的目标就是：希望模型学到的分布 $P_{model}$ 与训练集的分布 $P_{train}$ 一致。 然后剩余的问题就是如何评估两个分布是否一致？答案是使用KL散度进行评估。因为KL散度的定义就是衡量两个概率分布 $p$ 和 $q$ 的差异。 两个分布越相近，KL散度越小；两个分布的差异越大，KL散度也越大；当两个分布相同时，KL散度为0。 3、熵、KL散度、交叉熵先对这三个概念给出一个通俗但不严谨的描述： 熵：可以表示一个事件 A 的自信息量，即 A 包含多少信息； KL散度：可以表示从事件 A 的角度看，事件 B 有多大的不同； 交叉熵：可以表示从事件 A 的角度看，如何描述事件 B； 下面使用数据公示给出这三个概念的严谨的表示： 熵： $$H(p) = - \\sum_i p_i \\log p_i$$ KL散度： $$D_{KL}(p||q) = \\sum_i p_i \\log \\frac{p_i}{q_i} = \\sum_i p_i \\log p_i - \\sum_i p_i \\log q_i$$ 交叉熵： $$H(p||q) = - \\sum_i p_i \\log q_i$$ 注意熵和交叉熵公式中都带有一个负号，而KL散度的公式中并没有负号； 分析一下上面的KL散度的公式，左侧项 $\\sum_i p_i \\log p_i$ 很像是熵的公式，即 $-H(p)$；右侧项 $-\\sum_i p_i \\log q_i$ 就是交叉熵的公式，即 $H(p||q)$；所以会推导出如下公式： $$D_{KL}(p||q) = H(p||q) - H(p)$$ 即从公式上来说：KL散度等于交叉熵减熵。 4、机器学习中为什么多用交叉熵而不是KL散度在第二部分的描述中已经很清晰的提到：机器学习就是将模型分布 $P_{model}$ 学到与训练集分布 $P_{train}$ 一致的过程。而衡量两个分布是否一致最直接的评估方式就是KL散度，那么为什么机器学习中常用交叉熵而不是KL散度？ 在第三部分的最后推导出了一个公式，再次记录如下： $$ \\begin{equation} \\begin{split} D_{KL}(p||q) &= \\sum_i p_i \\log \\frac{p_i}{q_i} \\\\ &= \\big[ -\\sum_i p_i \\log q_i \\big] - \\big[ -\\sum_i p_i \\log p_i \\big] \\\\ &= H(p||q) - H(p) \\end{split} \\end{equation} $$ 将上述公式放到机器学习这个具体应用场景中，公式中的概率分布 $q$ 就是需要学习才能得到的模型分布 $P_{model}$，公式中的概率分布 $p$ 就是训练集分布 $P_{train}$。 我们知道在机器学习中，训练集是固定的，所以训练集的熵 $H(p)$ 也是固定的，不随着模型的优化过程而变化。即在机器学习这个应用场景下 $H(p)$ 是常数。此时使用 $D_{KL}(p||q)$ 对模型优化与使用 $H(p||q)$ 对模型优化是等价的。由于使用交叉熵 $H(p||q)$ 时还能少计算一项，节省计算资源，所以机器学习中一般较多情况使用交叉熵。 5、KL散度的性质最后记录一下KL散度的两个数学性质： 正定性：$D_{KL}(p||q) \\geqslant 0$ 不对称性：$D_{KL}(p||q) != D_{KL}(q||p)$ 由于KL散度不具有对称性，所以KL散度不是一种距离（度量）。 一般来说距离（度量）要满足3个条件：正定性、对称性、三角不等式； Reference https://blog.csdn.net/qq_40406773/article/details/80630280 https://zhuanlan.zhihu.com/p/39682125 https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/ https://www.zhihu.com/question/336677048 https://www.zhihu.com/question/65288314/answer/244557337","link":"/daka/posts/b53fc86/"},{"title":"线性回归 - 002","text":"2022-06-16 打卡；","link":"/daka/posts/83cd643a/"},{"title":"线性回归 - markdown","text":"2022-06-16 打卡（晚间）； 1、问题描述给定数据集 $D={(\\overrightarrow{x^{(1)}}, y^{(1)}), (\\overrightarrow{x^{(2)}}, y^{(2)}), …, (\\overrightarrow{x^{(n)}}, y^{(n)})}$ 其中： $\\overrightarrow{x^{(i)}}=(x_1^{(i)}, x_2^{(i)}, x_3^{(i)}, …, x_d^{(i)})^T \\in R^d$ 表示第 $i$ 条数据的特征； $d$ 表示每条数据特征的维度； $n$ 表示总的数据量； $y^{(i)} \\in R$ 表示第 $i$ 条数据的标签； 符号说明：右上角的角标代表第几条数据，右下角的角标代表第几个特征，比如 $x_j^{(i)}$ 表示第 $i$ 条数据的第 $j$ 个特征的值； 2、建立模型 $$ \\begin{equation} \\begin{split} h_ \\theta (\\overrightarrow{x^{(i)}}) &= \\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} + ... + \\theta_d x_d^{(i)} \\\\ &= \\sum_{j=0}^d \\theta_j x_j^{(i)} \\end{split} \\end{equation} $$ 上式中 $\\theta_0$ 是bias，为了便于计算，假设 $x_0^{(i)}=1$，则可以直接将 $\\theta_0$ 合并到 $\\sum_{j=0}^d \\theta_j x_j^{(i)}$ 中。 一般的，模型的输出会记为 $\\hat{y}^{(i)}$，则有： $$\\hat{y}^{(i)}=h_ \\theta (\\overrightarrow{x^{(i)}})$$ 3、代价函数使用最小二乘（least mean squares）作为代价函数，如下： $$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (h_ \\theta (\\overrightarrow{x^{(i)}}) - y_{(i)})^2$$ 4、梯度下降求解4.1 求解梯度 $$ \\begin{equation} \\begin{split} \\frac{\\partial}{\\partial \\theta_j} J(\\theta) &= \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} \\sum_{i=1}^n (h_ \\theta (\\overrightarrow{x^{(i)}}) - y_{(i)})^2 \\\\ &= \\frac{1}{2} \\cdot 2 \\cdot \\sum_{i=1}^n (h_ \\theta (\\overrightarrow{x^{(i)}}) - y_{(i)}) \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_ \\theta (\\overrightarrow{x^{(i)}}) - y_{(i)}) \\\\ &= \\sum_{i=1}^n (h_ \\theta (\\overrightarrow{x^{(i)}}) - y_{(i)}) x_j \\end{split} \\end{equation} $$ 4.2 梯度下降对于权重 $\\theta_j$ 每次迭代更新的公式为： $$ \\begin{equation} \\begin{split} \\theta_j &:= \\theta_j - \\gamma \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\\\ &:= \\theta_j - \\gamma \\sum_{i=1}^n (h_ \\theta (\\overrightarrow{x^{(i)}}) - y_{(i)}) x_j \\end{split} \\end{equation} $$ 其中 $\\gamma$ 为学习率；符号 $:=$ 表示赋值运算符； 4.3 随机梯度下降在梯度下降中，每次对 $\\theta_j$ 的更新都是求解出 $n$ 条数据在 $\\theta_j$ 上的所有梯度之后再进行更新。如果每条数据求解出梯度之后都对 $\\theta_j$ 更新一次，则为随机梯度下降，公式如下： $$\\theta_j := \\theta_j - \\gamma (h_ \\theta (\\overrightarrow{x^{(i)}}) - y_{(i)}) x_j$$ 5、直接求解析解","link":"/daka/posts/f4ca54ac/"},{"title":"正则化 - 002","text":"2022-06-26 打卡；","link":"/daka/posts/db115068/"},{"title":"正则化 - 001","text":"2022-06-26 打卡；","link":"/daka/posts/421801d2/"},{"title":"线性回归 - 001","text":"2022-06-15 打卡； 说明： 下面的这个线性回归并不是今天写的，今天只是对之前写的一些东西做了整理，后面会将之前写的东西重新梳理一遍，重新手写一遍，并转换成markdown格式。今天整理出来的之前开始但是没有完成的有4项工作： 线性归回； 前向传播与反向传播； 各种距离； SimCSE（对比学习）；","link":"/daka/posts/1ac43580/"},{"title":"Jacobi矩阵 - markdown","text":"2022-06-26 打卡； 1、Jocabi矩阵假设 $F : R_n \\rightarrow R_m$ 是一个从 $n$ 维欧氏空间映射到 $m$ 维欧氏空间的函数。这个函数由 $m$ 个实函数组成： $$ \\begin{equation} \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, ..., x_n) \\\\ ... \\\\ y_m = f_m(x_1, x_2, ..., x_n) \\\\ \\end{cases} \\end{equation} $$ 这些实函数的偏导数可以组成一个 $m*n$ 的矩阵，这个矩阵就是Jocabi矩阵： $$ \\begin{equation} J_F(x_1, x_2, ..., x_n) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & ... & \\frac{\\partial f_2}{\\partial x_n} \\\\ ... \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & ... & \\frac{\\partial f_m}{\\partial x_n} \\\\ \\end{bmatrix} \\end{equation} $$ 在向量分析中，Jacobi矩阵是一阶偏导数以一定的方式排列成的矩阵； 当 $m$ 等于 $n$ 时，Jacoba矩阵就变成了一个方阵，该方阵的行列式即为Jacobi行列式。 2、几何理解Jocabi矩阵的重要性在于它体现了一个可微方程与给定点的最优线性逼近； 如果 $p$ 是 $R^n$ 中的一个点，$F$ 在 $p$ 点可微，那么Jacobi矩阵 $J_F(p)$ 是在这点的导数；在此情况下 $J_F(p)$ 这个线性映射即为 $F$ 在点 $p$ 附近的最优线性逼近，也就是说当点 $x$ 足够靠近点 $p$ 时，我们有： $$\\begin{equation}F(x) \\approx F(p) + J_p(p)(x-p)\\end{equation}$$ 关于这个最优线性逼近，写一下从几何方面对他的理解： 我们知道，在几何中：矩阵表示的是线性变换，比如平移、旋转、缩放。当给定的矩阵中每个元素都是常数时，该矩阵对应的线性变换就是固定的，比如下图中的变换对应的矩阵为 $\\begin{bmatrix}1 & 0 \\\\ 1 & 1\\end{bmatrix}$： 上图中左侧的正方形经过变换矩阵$\\begin{bmatrix}1 & 0 \\\\ 1 & 1\\end{bmatrix}$之后得到上图右侧的平行四边形； 与上述这种变换的区别是，Jacobi矩阵中的每个元素并不都是常数，而是与 $x$ 相关的，当给定一个具体的 $x$ 之后，就可求出Jacobi矩阵中每个元素的值，此时在给定的 $x$ 的极小邻域内是一个线性变换。这就是：Jocabi矩阵体现了一个可微方程与给定点的最优线性逼近。 关于Jacobi矩阵在几何方便的理解可以参考bilibili上的视频：《雅可比矩阵是什么东西》； 3、另一个角度引出Jocabi矩阵Jocabi矩阵是某种映射函数的导数，只不过这种映射函数比较复杂，下面从简单到复杂把几种映射函数和其导数都列一下。 3.1 单函数单变量假设 $F : R \\rightarrow R$ 是一个从一维欧氏空间映射到一维欧氏空间的函数，这个函数的形式为 $y=f(x)$，则其导数为： $$\\begin{equation}\\frac{df}{dx}\\end{equation}$$ 3.2 单函数多变量假设 $F : R_n \\rightarrow R$ 是一个从 $n$ 维欧氏空间映射到一维欧氏空间的函数，这个函数的形式为 $y=f(x_1, x_2, …, x_n)$，则其导数为： $$\\begin{equation}[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, …, \\frac{\\partial f}{\\partial x_n}]\\end{equation}$$ PS：这里没有考虑应该是行向量还是列向量； 3.3 多函数多变量假设 $F : R_n \\rightarrow R_m$ 是一个从 $n$ 维欧氏空间映射到 $m$ 维欧氏空间的函数，这些个函数的形式为 $$ \\begin{equation} \\begin{cases} y_1 = f_1(x_1, x_2, ..., x_n) \\\\ y_2 = f_2(x_1, x_2, ..., x_n) \\\\ ... \\\\ y_m = f_m(x_1, x_2, ..., x_n) \\\\ \\end{cases} \\end{equation} $$ 则其导数为： $$ \\begin{equation} \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & ... & \\frac{\\partial f_2}{\\partial x_n} \\\\ ... \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & ... & \\frac{\\partial f_m}{\\partial x_n} \\\\ \\end{bmatrix} \\end{equation} $$ 也就是Jocabi矩阵。 Reference 雅可比矩阵（整理）: https://www.cnblogs.com/rswss/p/11440144.html 如何理解雅克比矩阵: https://www.zhihu.com/question/22586361 雅可比矩阵是什么东西: https://www.bilibili.com/video/BV1NJ411r7ja/","link":"/daka/posts/a792e00f/"},{"title":"常用距离 - 001","text":"2022-06-26 打卡；","link":"/daka/posts/7b5c5552/"},{"title":"常用距离 - markdown","text":"2022-06-26 打卡； 在 $d$ 维空间中的两点 $x=(x_1, x_2, …, x_d)$ 和 $y=(y_1, y_2, …, y_n)$，求这两点之间距离。 常用的公式有： 1、$L_p$ 距离$L_p$ 距离也称闵可夫斯基距离，公式为： $$\\begin{equation}L_p(x, y) = (\\sum_{i=1}^d |x_i - y_i|^p)^{\\frac{1}{p}}\\end{equation}$$ 2、$L_1$ 距离$L_1$ 距离也称曼哈顿距离，公式为： $$\\begin{equation}L_p(x, y) = \\sum_{i=1}^d |x_i - y_i|\\end{equation}$$ 3、$L_2$ 距离$L_2$ 距离也称欧式距离，公式为： $$\\begin{equation}L_p(x, y) = (\\sum_{i=1}^d |x_i - y_i|^2)^{\\frac{1}{2}}\\end{equation}$$ 4、$L_{\\infty}$ 距离$L_{\\infty}$ 距离也称切比雪夫距离，公式为： $$\\begin{equation}L_{\\infty}(x, y) = \\max_{i} |x_i - y_i| \\end{equation}$$ 以上这几种距离的关系其中，对于 $L_p$ 距离，当$p$取不同的值时，即可取到另外的3种距离； 先看两个极端，即 $p=1$ 和 $p=\\infty$ 时的特点，然后再看 $p$ 在不同取值下各种距离之间的关系： 当 $p=1$时，即 $L_1$ 距离是每个特征维度的距离之和； 当 $p=\\infty$ 时，即$L_{\\infty}$ 距离是直接取最大的那个特征维度的距离，其他的维度的距离直接忽略； 可以看出：当 $p$ 由 $1$ 到 $\\infty$ 渐变的过程中，就是给逐渐给距离较大的那些特征维度更大权重的过程。尤其是当 $p=\\infty$，就只看距离最大的那个特征，其他的特征全部都忽略掉。 5、余弦距离公式为： $$\\begin{equation}\\cos x= \\frac{x_1 * y_1 + x_2 * y_2 + … + x_n * y_n}{\\sqrt{x_1^2 + y_1^2} * \\sqrt{x_2^2 + y_2^2} * … * \\sqrt{x_n^2 + y_n^2}}\\end{equation}$$ 余弦距离的一些特点： 取值范围在-1到1之间，$[-1, 1]$； 具有周期性； 连续且处处可微；","link":"/daka/posts/c05b9662/"},{"title":"正则化 - markdown","text":"2022-06-26 打卡； 说明：在该篇文章中所有的推导都忽略了偏置项 bias； 1、$L_2$正则正则化项为 $\\Omega (\\theta) = \\frac{1}{2}||\\theta||^2_2$，系数 $\\frac{1}{2}$ 是为了求导时得到的系数为 $1$； $L_2$ 正则化能够使参数 $\\theta$ 的方差更接近0； 目标函数： $$\\begin{equation}J(\\theta)=L(\\theta)+ \\lambda \\Omega(\\theta) = L(\\theta) + \\frac{1}{2}\\lambda ||\\theta||^2_2\\end{equation}$$ 梯度为： $$\\begin{equation}\\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} L(\\theta) + \\lambda \\theta\\end{equation}$$ 梯度下降过程如下，这里的 $\\gamma$ 为学习率： $$\\begin{equation}\\theta \\leftarrow \\theta - \\gamma (\\nabla_{\\theta} L(\\theta) + \\lambda \\theta)\\end{equation}$$ 对上述梯度下降过程整理一下得： $$\\begin{equation}\\theta \\leftarrow (1-\\gamma \\lambda)\\theta - \\gamma \\nabla_{\\theta} L(\\theta)\\end{equation}$$ 在不使用 $L_2$ 正则化的情况下，梯度下降的公式为： $$\\begin{equation}\\theta \\leftarrow \\theta - \\gamma \\nabla_{\\theta} L(\\theta)\\end{equation}$$ 对比公式$(4)$和公式$(5)$可知，$L_2$正则化对梯度更新的影响是：每一步执行更新前，会对权重向量乘以一个常数因子来收缩权重向量，使参数 $\\theta$ 的方差更接近0，因此$L_2$也被称为权重衰减； 2、$L_1$正则正则化项为 $\\Omega(\\theta) = ||\\theta||_1 = \\sum_{i=1}^d |\\theta_i|$，即各个参数的绝对值之和； 目标函数： $$\\begin{equation}J(\\theta)=L(\\theta)+\\Omega(\\theta)=L(\\theta)+\\lambda ||\\theta||_1\\end{equation}$$ 梯度： $$\\begin{equation}\\nabla_{\\theta} J(\\theta)=\\nabla_{\\theta} L(\\theta) + \\lambda \\cdot \\text{sign}(\\theta)\\end{equation}$$ 其中 $\\text{sign}(\\cdot)$ 表示取自变量的符号； 梯度下降过程如下，这里的 $\\gamma$ 为学习率： $$\\begin{equation}\\theta \\leftarrow \\theta - \\gamma \\cdot(\\nabla_{\\theta} L(\\theta) + \\lambda \\cdot \\text{sign}(\\theta))\\end{equation}$$ 对上述梯度下降过程整理一下得： $$\\begin{equation}\\theta \\leftarrow (\\theta - \\gamma \\cdot \\lambda \\cdot \\text{sign}(\\theta)) - \\gamma \\cdot \\nabla_{\\theta}L(\\theta)\\end{equation}$$ 在不使用 $L_1$ 正则化的情况下，梯度下降的公式为： $$\\begin{equation}\\theta \\leftarrow \\theta - \\gamma \\nabla_{\\theta} L(\\theta)\\end{equation}$$ 对比公式$(9)$和公式$(10)$可知: 上面讨论过 $L_2$ 正则化对梯度更新的影响是：给每个权重值乘上一个常数因子，线性的缩放每个权重值； $L_1$正则化对梯度更新的影响是：给每个权重值减去一个与 $\\text{sign}(\\theta_i)$ 同符号的常数因子； 特别说明：对于带有 $L_1$ 正则的目标函数，由于其不是处处可导，所以一般不使用梯度下降法进行求解，这里只是和 $L_2$ 正则做一个类似的讨论。在本文的最后一部分会介绍坐标下降法，是更常用的用于求解带有 $L_1$ 正则的目标函数的方法； 3、Lasso回归与岭回归3.1 Lasso回归与岭回归的定义在线性回归的目标函数上添加上 $L_1$ 或 $L_2$ 正则化项，则可以得到Lasso回归和岭回归，其公式如下： Lasso回归： $$\\begin{equation}J(\\theta)=L(\\theta)+ \\lambda ||\\theta||_1 =\\frac{1}{2}\\sum_{i=1}^n (h_{\\theta}(x_i) - y_i)^2 + \\lambda ||\\theta||_1\\end{equation}$$ 岭回归： $$\\begin{equation}J(\\theta)=L(\\theta)+ \\lambda \\cdot \\frac{1}{2} ||\\theta||_2^2= \\frac{1}{2}\\sum_{i=1}^n (h_{\\theta}(x_i) - y_i)^2 + \\lambda \\cdot \\frac{1}{2} ||\\theta||_2^2\\end{equation}$$ 3.2 $L_1$为何能做特征筛选关于 $L_1$ 和 $L_2$ 有一个常见的结论是： $L_1$：能够使权重值中的一些特征趋于0，因此可以用来做特征筛选； $L_2$：能够使权重值中的所有特征的方差趋于0； 这一部分讨论一下为什么 $L_1$ 能够使权重值中的一些特征趋于0； Lasso回归与岭回归的目标函数都是拉格朗日格式，其中 $\\lambda$ 是KKT乘子，所以可以将其改写为带有约束条件的最优化问题。 Lasso回归： $$ \\begin{equation} \\begin{split} & \\min_{\\theta} \\frac{1}{2} \\sum_{i=1}^n (h_{\\theta}(x_i) - y_i)^2 \\\\ & s.t. \\quad ||\\theta||_1 \\leqslant t \\end{split} \\end{equation} $$ 岭回归： $$ \\begin{equation} \\begin{split} & \\min_{\\theta} \\frac{1}{2} \\sum_{i=1}^n (h_{\\theta}(x_i) - y_i)^2 \\\\ & s.t. \\quad \\frac{1}{2}||\\theta||_2^2 \\leqslant t \\end{split} \\end{equation} $$ 其中 $t$ 表示正则化的力度，$t$ 越小，正则化力度越大，也对应原目标函数中的 $\\lambda$ 越大。 下面通过画图来理解。 如上图所示，Lasso回归（即公式$(13)$）的约束条件为左图中灰色的方形区域；岭回归（即公式$(14)$）的约束条件为右图中灰色的圆形区域；两图中右上方的椭圆线为损失函数 $L(\\theta)$ 的等高线，损失函数 $L(\\theta)$ 在椭圆的中心处取得最小值。 既要满足方形/圆形的灰色区域的约束，又要尽量取最小值，可知上述两个带约束的最优化问题的最优解都在：等高线与约束区域边界的交点处；即两个红色箭头所指的交点处。 由于 $L_1$ 正则对应的约束区域是方形的，椭圆形的等高线与方形区域边界的交点更容易出现在该方形区域的顶点上，也就是坐标轴上。而这些坐标轴上的点仅当前坐标轴对应的维度非0，其他维度取值都为0。所以相比于 $L_2$，$L_1$更适合做特征选择。 4、带有$L_2$正则的目标函数的求解4.1 公式由于 $L_2$ 正则本身连续且处处可微，所以直接使用梯度下降法即可进行求解。 在第一部分的讨论中，已经求得了带有 $L_2$ 正则的目标函数的梯度下降过程公式： $$\\begin{equation}\\theta \\leftarrow \\theta - \\gamma \\cdot (\\nabla_{\\theta} L(\\theta) + \\lambda \\theta)\\end{equation}$$ 这个公式很简单，下面看一下在Pytorch中是如何实现$L_2$正则的功能的。 4.1 Pytorch中$L_2$的实现4.1.1 调用方式在 pytorch 中L2正则是通过 weight decay 在优化器中实现的，只需要在初始化优化器时指定哪些参数需要L2正则，哪些参数不需要L2正则即可。如下所示： 123456789101112131415weight_decay = 0.01learning_rate = 0.00005no_decay = [&quot;bias&quot;, &quot;LayerNorm.weight&quot;]optimizer_grouped_parameters = [ { &quot;params&quot;: [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], &quot;weight_decay&quot;: weight_decay, }, { &quot;params&quot;: [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], &quot;weight_decay&quot;: 0.0, },]optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate) 4.1.2 源码以最简单的SGD优化器来看一下L2正则在优化器中具体是如何实现的： 12345678910111213141516171819class SGD(Optimizer): def step(self,): for group in self.param_groups: lr = group[&quot;lr&quot;] # 学习率 weight_decay = group[&quot;weight_decay&quot;] # weight decay for p in group[&quot;params&quot;]: if p.grad is None: # 如果当前参数梯度为None，则不需要更新 continue d_p = p.grad # 梯度 if weight_decay != 0: # 在原梯度的基础上加上 (weight_decay * 权重) d_p.add_(weight_decay, p.data) # 将（梯度 * -学习率）更新到权重参数上；当 weight_decay 不等 # 于0时，这里的梯度 d_p 已经加上了(weight_decay * 权重) p.data.add_(-lr, d_p) 再放一下梯度下降过程的公式，对着公式来看代码： $$\\begin{equation}\\theta \\leftarrow \\theta - \\gamma \\cdot (\\nabla_{\\theta} L(\\theta) + \\lambda \\cdot \\theta)\\end{equation}$$ 公式中的 $\\lambda$ 对应代码中的 weight_decay； 公式中的 $\\gamma$ 对应代码中的 lr； 代码中的注释很详细，不再赘述。 5、带有$L_1$正则的目标函数的求解下面使用坐标轴下降法对带有 $L_1$ 正则的目标函数进行求解； 梯度下降法是沿着梯度的负方向下降；坐标轴下降法是沿着坐标轴方向下降；二者的相似点是都是迭代法； 5.1 优化问题先在这里列一下模型和目标函数，然后描述如何使用坐标轴下降法对该优化问题进行求解； 模型为：$y = h_{\\theta(x)}$，其中权重参数 $\\theta$ 的维度为 $d$，即 $\\theta = (\\theta_1, \\theta_2, …, \\theta_d)$ 损失函数选取MSE，则目标函数为： $$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^n (y_i - \\sum_{j=1}^d x_{ij} \\cdot \\theta_j)^2 + \\lambda \\sum_{j=1}^d |\\theta_j|$$ 5.2 坐标轴下降法坐标轴下降算法过程分为三个步骤： 对权重进行随机初始化；随机初始化的权重记作 $\\theta^{(0)}=(\\theta_1^{(0)}, \\theta_2^{(0)}, …, \\theta_d^{(0)})$；这里右上角的角标 $\\cdot^{(0)}$ 表示当前是第0轮迭代，右下角的角标表示的是权重参数的第几个维度； 进行第 $k$ 轮迭代； 第 $k$ 轮迭代前：$\\theta^{(k-1)} = (\\theta_1^{(k-1)}, \\theta_2^{(k-1)}, …, \\theta_d^{(k-1)})$ 第 $k$ 轮迭代后：$\\theta^{(k)} = (\\theta_1^{(k)}, \\theta_2^{(k)}, …, \\theta_d^{(k)})$ 在第 $k$ 轮迭代中，会从坐标轴1逐步迭代到坐标轴 $d$（即迭代权重参数的每个维度），对 $\\theta^{(k-1)}$ 的 $d$ 个维度逐步迭代的公式如下： $$\\begin{equation}\\begin{split} & \\theta_1^{(k)} = \\arg \\min_{\\theta_1} J(\\theta_1, \\theta_2^{(k-1)}, ..., \\theta_d^{(k-1)}) \\\\ & \\theta_2^{(k)} = \\arg \\min_{\\theta_2} J(\\theta_1^{(k)}, \\theta_2, \\theta_3^{(k-1)}, ..., \\theta_d^{(k-1)}) \\\\ & ... \\\\ & \\theta_d^{(k)} = \\arg \\min_{\\theta_d} J(\\theta_1^{(k)}, \\theta_2^{(k)}, ..., \\theta_{d-1}^{(k)}, \\theta_d) \\end{split}\\end{equation}$$ 注意在上述公式（17）中，第一个式子的等号右边只有 $\\theta_1$ 为变量，其它都是常量；第二个式子的等号右边只有 $\\theta_2$ 为变量，其它都是常量；最后一个式子的等号右边只有 $\\theta_d$ 是变量，其他都是常量； 公式（17）中的 $d$ 个式子可以合并用如下一个式子进行表示： $$\\theta_l^{(k)} = \\arg \\min_{\\theta_l} J(\\theta_1^{(k)}, \\theta_2^{(k)}, …, \\theta_{l-1}^{(k)}, \\theta_{l}, \\theta_{l+1}^{(k-1)}, …, \\theta_d^{(k-1)}), \\qquad l \\in \\{1, 2, …, d\\}$$ 在该式的等号右边只有 $\\theta_l$ 是变量，其它的像 $\\theta_1^{(k)}$、$\\theta_2^{(k)}$、…、$\\theta_{l-1}^{(k)}$、$\\theta_{l+1}^{(k-1)}$、…、$\\theta_d^{(k-1)}$都是常量； 我们现在先假设该式是容易求解的，先看完坐标轴下降法的整个算法流程；至于该式的具体解法在下一小节说明； 终止条件：第 $k$ 次迭代完成后，检查 $\\theta^{(k)}$ 与 $\\theta^{(k-1)}$ 在各个维度上的变化情况，如果各个维度的变化都足够小，那么终止迭代，$\\theta^{(k)}$ 即为最终结果；否则重复第2步，继续迭代； 以上就是坐标轴下降法：先随机初始化全中矩阵；然后对每个维度的参数进行优化，在优化第 $l$ 个维度时，将其它维度视为常量；待每个维度都优化一轮之后，检查是否可以终止：若可以终止，则优化结束，否则重复上述优化步骤； 5.3 求解 $\\arg \\min_{\\theta_l} J$在上一小节遗留了一个问题，就是如何求解下式，在这一小节对该式进行求解。 $$\\begin{equation}\\theta_l^{(k)} = \\arg \\min_{\\theta_l} J(\\theta_1^{(k)}, \\theta_2^{(k)}, …, \\theta_{l-1}^{(k)}, \\theta_{l}, \\theta_{l+1}^{(k-1)}, …, \\theta_d^{(k-1)})\\end{equation}$$ 5.3.1 问题描述再次描述一个整个问题，如下： 模型为：$y = h_{\\theta(x)}$，其中权重参数 $\\theta$ 的维度为 $d$，即 $\\theta = (\\theta_1, \\theta_2, …, \\theta_d)$ 目标函数为： $$\\begin{equation}J(\\theta) = \\frac{1}{2}\\sum_{i=1}^n (y_i - \\sum_{j=1}^d x_{ij} \\cdot \\theta_j)^2 + \\lambda \\sum_{j=1}^d |\\theta_j|\\end{equation}$$ 求解： $$\\begin{equation}\\theta_l^{(k)} = \\arg \\min_{\\theta_l} J(\\theta_1^{(k)}, \\theta_2^{(k)}, …, \\theta_{l-1}^{(k)}, \\theta_{l}, \\theta_{l+1}^{(k-1)}, …, \\theta_d^{(k-1)})\\end{equation}$$ 其中 $n$ 表示数据集中的样本数量；$d$ 表示每条样本的维度，同时也是权重参数 $\\theta$ 的维度；$\\lambda$ 是超参数； 求解上式，我们采用导数等于 $0$ 的方式。所以下面将分为两部分说明：一部分是求导；另一部分是令导数等于 $0$ 求解出 $\\theta_l$； 5.3.2 求导 $$\\begin{equation}\\begin{split} & \\frac{\\partial}{\\partial \\theta_l} J(\\theta_1^{(k)}, \\theta_2^{(k)}, ..., \\theta_{l-1}^{(k)}, \\theta_{l}, \\theta_{l+1}^{(k-1)}, ..., \\theta_d^{(k-1)}) \\\\ = & \\sum_{i=1}^n (y_i - \\sum_{j=1}^d x_{ij} \\cdot \\theta_j) \\cdot (-x_{il}) + \\lambda \\frac{\\partial}{\\partial \\theta_l} |\\theta_l| \\\\ = & \\sum_{i=1}^n (y_i - \\sum_{j \\neq l}^d x_{ij} \\cdot \\theta_j - x_{il} \\cdot \\theta_l) \\cdot (-x_{il}) + \\lambda \\frac{\\partial}{\\partial \\theta_l} |\\theta_l| \\\\ = & - \\sum_{i=1}^n (y_i - \\sum_{j \\neq l}^d x_{ij} \\cdot \\theta_j) \\cdot x_{il} + \\sum_{i=1}^n x_{il}^2 \\cdot \\theta_l + \\lambda \\frac{\\partial}{\\partial \\theta_l} |\\theta_l| \\end{split}\\end{equation}$$ 在上述推导过程中，后两步变换过程主要目的是将变量提出来，在上式中只有 $\\theta_l$ 是变量，其他的都是常量。 为了后面书写方便，记： $$\\begin{equation}\\begin{split} & r_l = \\sum_{i=1}^n (y_i - \\sum_{j\\neq l}^d x_{ij} \\cdot \\theta_j) \\cdot x_{il} \\\\ & z_l = \\sum_{i=1}^n x_{il}^2 \\end{split}\\end{equation}$$ 再次强调，由于 $x_{ij}$、$y_i$、$\\theta_j(j \\neq l)$ 都是常量，所以这里的 $r_l$ 和 $z_l$ 也都是常量；于是公式（21）就变成了如下形式： $$\\begin{equation}\\frac{\\partial J(…)}{\\partial \\theta_l} = -r_l + z_l \\cdot \\theta_l + \\lambda \\frac{\\partial}{\\partial \\theta_l} |\\theta_l|\\end{equation}$$ 这里涉及到了对绝对值函数进行求导，涉及次梯度的概念。在此，只放一个结论，不过多讨论。对于函数 $f(x)=|x|$，其导数为： $$f^{\\prime}(x) = \\begin{cases} 1, &x > 0 \\\\ [-1, 1], &x = 0 \\\\ -1, &x < 0 \\end{cases}$$ 根据绝对值函数导数的结论，对公式（23）继续求导可得： $$\\begin{equation} \\frac{\\partial J(...)}{\\partial \\theta_l} =\\begin{cases} -r_l + z_l \\theta_l + \\lambda, &\\theta_l > 0 \\\\ [-r_l + z_l \\theta_l - \\lambda, -r_l + z_l \\theta_l + \\lambda], &\\theta_l = 0 \\\\ -r_l + z_l \\theta_l - \\lambda, &\\theta_l < 0 \\end{cases}\\end{equation}$$ 至此，求导完成，下面令导数等于0，求解 $\\theta_l$； 5.3.3 令导数等于0求解 $\\theta_l$由于上一步中最后求得的导数为分段函数，这里也分段进行求解； 当 $\\theta_l &gt; 0$ 时： 有 $-r_l + z_l \\theta_l + \\lambda = 0$，解得 $\\theta_l = \\frac{r_l - \\lambda}{z_l}$ 由于 $\\theta_l=\\frac{r_l - \\lambda}{z_l}&gt;0$ 且 $z_l = \\sum_{i=1}^n x_{il}^2 \\geqslant 0$，可推导出 $r_l &gt; \\lambda$ 当 $\\theta_l &lt; 0$ 时： 有 $-r_l + z_l \\theta_l - \\lambda = 0$，解得 $\\theta_l = \\frac{r_l + \\lambda}{z_l}$ 由于 $\\theta_l=\\frac{r_l + \\lambda}{z_l}&gt;0$ 且 $z_l = \\sum_{i=1}^n x_{il}^2 \\geqslant 0$，可推导出 $r_l &lt; -\\lambda$ 当 $\\theta_l = 0$ 时： 有 $0 \\in [-r_l - \\lambda, -r_l + \\lambda]$，可推导出 $-\\lambda \\leqslant r_l \\leqslant \\lambda$ 将三部分合到一起，即： $$\\begin{equation}\\theta_l = \\begin{cases} \\frac{r_l - \\lambda}{z_l}, &r_l > \\lambda \\\\ 0, &-\\lambda \\leqslant r_l \\leqslant \\lambda \\\\ \\frac{r_l + \\lambda}{z_l}, &r_l < -\\lambda \\end{cases}\\end{equation}$$ 至此对 $\\arg \\min_{\\theta_l} J(\\theta_1^{(k)}, \\theta_2^{(k)}, …, \\theta_{l-1}^{(k)}, \\theta_{l}, \\theta_{l+1}^{(k-1)}, …, \\theta_d^{(k-1)})$ 的求解完成。 5.3.4 伪代码实现前面已经说过多次，$r_l$ 和 $z_l$ 都是常量，可通过数据集 $\\{(x_1,y_1), (x_2, y_2), …, (x_n, y_n)\\}$ 与 除了第 $l$ 维以外的权重参数 $\\theta_1^{(k)}$、$\\theta_2^{(k)}$、…、$\\theta_{l-1}^{(k)}$、$\\theta_{l+1}^{(k-1)}$、…、$\\theta_d^{(k-1)}$ 求解出来； 据此给出伪代码如下： 1234function calculate_theta_l(): r_l = ... # 前面已解释了r_l可直接求得 z_l = ... # 前面已解释了z_l可直接求得 theta_l = max((r_l - lambda)/z_l, 0) + min((r_l + lambda)/z_l, 0) # 上面最后推导出来的分段函数可以整合这一行伪代码 Refrence 正则化：http://www.huaxiaozhuan.com/深度学习/chapters/3_regularization.html Lasso回归和岭回归：https://www.cnblogs.com/wuliytTaotao/p/10837533.html Lasso回归算法： 坐标轴下降法与最小角回归法小结：https://www.cnblogs.com/pinard/p/6018889.html LASSO的坐标下降法求解：https://zhuanlan.zhihu.com/p/429541451","link":"/daka/posts/4cc516a3/"},{"title":"均方差损失和交叉熵损失 - 002","text":"2022-06-06 打卡；","link":"/daka/posts/b35257c4/"},{"title":"交叉熵损失与均方差损失的区别","text":"2022-06-07 打卡； 说明：本文讨论的是在做分类任务时这两个损失的区别； 一、两者概念上的区别1、均方差损失(cross-entropy)：是求一个batch中n个样本的n个输出与其期望输出的差的平方的均值； 2、交叉熵损失(MSE)：用来评估当前训练得到的概率分布与真实分布的差异情况，它刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近； 二、两者更新速度上的区别下面分别对均方差损失、二分类交叉熵损失、多分类交叉熵损失，推导其梯度下降的过程，并对其在梯度下降过程中优化的速度进行分析。 2.1 均方差损失2.1.1 问题描述假设模型为单个神经元，单输入，单输出，二分类任务，使用sigmoid做二分类，则其前向传播过程为： $$z_i=wx_i+b$$ $$\\hat{y}_i=\\sigma(z_i)$$ 其损失为： $$L=\\frac{1}{N} \\sum_{i=1}^N L_i=\\frac{1}{N} \\sum_{i=1}^N \\frac{(y_i - \\hat{y}_i)^2}{2}$$ 符号说明： $N$：表示样本数量； $x_i$：表示第$i$条样本的输入； $y_i$：表示第$i$条样本的期望输出； $\\hat{y_i}$：表示第$i$条样本的真实输出； $z_i$：表示第$i$条样本只经过权重矩阵，未经过激活函数的中间结果； $L_i$：表示第$i$条样本的损失； $L$：表示所有$N$条样本的损失； $w$和$b$：表示权重举证； 2.1.2 求导 $$ \\begin{equation} \\begin{split} \\frac{\\partial L_i}{\\partial w} &= \\frac{\\partial L_i}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial w}\\\\ &= (\\hat{y}_i - y_i) \\sigma^{\\prime}(z_i) x_i \\\\ &= (\\hat{y}_i - y_i) \\sigma(z_i)(1 - \\sigma(z_i)) x_i \\\\ &= (\\hat{y}_i - y_i) \\hat{y}_i (1 - \\hat{y}_i) x_i \\end{split} \\end{equation} $$ $$ \\begin{equation} \\begin{split} \\frac{\\partial L_i}{\\partial b} &= \\frac{\\partial L_i}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial b}\\\\ &= (\\hat{y}_i - y_i) \\sigma^{\\prime}(z_i) \\\\ &= (\\hat{y}_i - y_i) \\sigma(z_i)(1 - \\sigma(z_i)) \\\\ &= (\\hat{y}_i - y_i) \\hat{y}_i (1 - \\hat{y}_i) \\end{split} \\end{equation} $$ 说明：上述推导中使用到了sigmoid的求导公式，若 $f(z)=\\frac{1}{1+e^{-z}}$，则有：$f^{\\prime}(z) = f(z)(1 - f(z))$ 求解出导数之后，权重值的更新比较简单，如下： $$w=w - \\eta \\frac{\\partial L}{\\partial w}=w - \\eta (\\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial w})$$ $$b=b - \\eta \\frac{\\partial L}{\\partial b}=b - \\eta (\\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial b})$$ 2.1.3 优化速度分析以对权重值 $w$ 的更新进行分析，权重值 $b$ 的更新速度分析方式完全相同； 再重新写一下更新权重时的公式，将梯度公式 $\\frac{\\partial L_i}{\\partial w} = (\\hat{y}_i - y_i) \\sigma^{\\prime}(z_i) x_i$ 代入到权重更新的公式，可以得到下式： $$\\begin{equation}\\begin{split}w&amp;=w - \\eta (\\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial w}) \\&amp;= w - \\eta \\Big[\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i) \\sigma^{\\prime}(z_i) x_i \\Big]\\end{split}\\end{equation}$$ 在上述公式中，$\\eta$ 是超参数，$N$ 为样本数量，$x_i$ 为输入的值，这几个参数都不需要考虑； 所以每次对权重更新的步长主要由 $(\\hat{y}_i - y_i) \\sigma^{\\prime}(z_i)$ 决定。 其中 $\\sigma^{\\prime}(z_i)$ 为 Sigmoid 函数的导数，下图是 Sigmoid 函数的图像，可以看出：当 $z_i$ 比较小或者比较大时，$\\sigma^{\\prime}(z_i)$ 的值（即下图曲线的斜率）都趋于0；也就是说当 $z_i$ 比较小或者比较大时，其梯度也是趋于0的。 而在做梯度下降时，希望当离目标较远时，每次更新的步长要大一些，可以快速收敛；当离目标较近时，每次更新的步长要小一些，可以防止在目标值附近震荡； 综上所述：在使用均方差损失时，当 $z_i$ 比较小或者比较大时，其梯度都是趋于0的。所以当梯度较小时，无法判断距离目标点较远，还是较近，优化起来比较困难。 2.2 二分类交叉熵损失常用的交叉熵损失又分为二分类的交叉熵损失（sigmoid）和多分类的交叉熵损失（softmax），这一部分先看二分类交叉熵损失，下一段再看多分类交叉熵损失； 2.2.1 问题描述假设模型为单个神经元，单输入，单输出，二分类任务，使用sigmoid做二分类，则其前向传播过程为： $$z_i=wx_i+b$$ $$\\hat{y}_i=\\sigma(z_i)$$ 其损失为： $$L_i=-\\Big[ y_i \\log \\hat{y}_i + (1-y_i)\\log (1-\\hat{y}_i) \\Big]$$ $$L=\\frac{1}{N} \\sum_{i=1}^N L_i$$ 符号说明： $N$：表示样本数量； $x_i$：表示第$i$条样本的输入； $y_i$：表示第$i$条样本的期望输出； $\\hat{y_i}$：表示第$i$条样本的真实输出； $z_i$：表示第$i$条样本只经过权重矩阵，未经过激活函数的中间结果； $L_i$：表示第$i$条样本的损失； $L$：表示所有$N$条样本的损失； $w$和$b$：表示权重举证； 2.2.2 求导 $$ \\begin{equation} \\begin{split} \\frac{\\partial L_i}{\\partial w} &= \\frac{\\partial L_i}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial w} \\\\ &= -(\\frac{y_i}{\\hat{y}_i} - \\frac{1-y_i}{1-\\hat{y}_i}) \\sigma^{\\prime}(z_i) x_i \\\\ &= -(\\frac{y_i}{\\hat{y}_i} - \\frac{1-y_i}{1-\\hat{y}_i}) \\sigma(z_i)(1-\\sigma(z_i)) x_i \\\\ &= -(\\frac{y_i}{\\hat{y}_i} - \\frac{1-y_i}{1-\\hat{y}_i}) \\hat{y}_i(1-\\hat{y}_i) x_i \\\\ &= -[y_i(1-\\hat{y}_i) - \\hat{y}_i(1-y_i)]x_i \\\\ &= - (y_i - y_i \\hat{y}_i - \\hat{y}_i + y_i \\hat{y}_i) x_i \\\\ &= (\\hat{y}_i - y_i) x_i \\end{split} \\end{equation} $$ $$ \\begin{equation} \\begin{split} \\frac{\\partial L_i}{\\partial b} &= \\frac{\\partial L_i}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial b} \\\\ &= -(\\frac{y_i}{\\hat{y}_i} - \\frac{1-y_i}{1-\\hat{y}_i}) \\sigma^{\\prime}(z_i) \\\\ &= -(\\frac{y_i}{\\hat{y}_i} - \\frac{1-y_i}{1-\\hat{y}_i}) \\sigma(z_i)(1-\\sigma(z_i)) \\\\ &= -(\\frac{y_i}{\\hat{y}_i} - \\frac{1-y_i}{1-\\hat{y}_i}) \\hat{y}_i(1-\\hat{y}_i) \\\\ &= -[y_i(1-\\hat{y}_i) - \\hat{y}_i(1-y_i)] \\\\ &= - (y_i - y_i \\hat{y}_i - \\hat{y}_i + y_i \\hat{y}_i) \\\\ &= (\\hat{y}_i - y_i) \\end{split} \\end{equation} $$ 说明：上述推导中使用到了sigmoid的求导公式，若 $f(z)=\\frac{1}{1+e^{-z}}$，则有：$f^{\\prime}(z) = f(z)(1 - f(z))$ 求解出导数之后，权重值的更新比较简单，如下： $$w=w - \\eta \\frac{\\partial L}{\\partial w}=w - \\eta (\\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial w})$$ $$b=b - \\eta \\frac{\\partial L}{\\partial b}=b - \\eta (\\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial b})$$ 2.2.3 优化速度分析以对权重值 $w$ 的更新进行分析，权重值 $b$ 的更新速度分析方式完全相同； 再重新写一下更新权重时的公式，将梯度公式 $\\frac{\\partial L_i}{\\partial w} = (\\hat{y}_i - y_i) x_i$ 代入到权重更新的公式，可以得到下式： $$\\begin{equation}\\begin{split}w&amp;=w - \\eta (\\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial w}) \\&amp;= w - \\eta \\Big[\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i) x_i \\Big]\\end{split}\\end{equation}$$ 在上述公式中，$\\eta$ 是超参数，$N$ 为样本数量，$x_i$ 为输入的值，这几个参数都不需要考虑； 所以每次对权重更新的步长主要由 $(\\hat{y}_i - y_i)$ 决定。 将均方差损失的梯度与二分类交叉熵损失的梯度放在一起比较一下就非常清晰了，如下： $$\\frac{\\partial L_i}{\\partial w}=(\\hat{y}_i - y_i)x_i \\qquad // \\text{二分类交叉熵损失的梯度}$$ $$\\frac{\\partial L_i}{\\partial w}=(\\hat{y}_i - y_i)\\sigma^{\\prime}(z_i) x_i \\qquad// \\text{均方差损失的梯度}$$ 可以看出两者之间仅差一个 $\\sigma^{\\prime}(z_i)$，所以有如下结论： 二分类交叉熵损失的梯度为期望输出与实际输出的差值，当距离目标点越远时，该差值越大，梯度越大；当距离目标点越近时，该差值越小，梯度越小； 均方差损失的梯度中由于包含了 $\\sigma^{\\prime}(z_i)$ 这一项，当距离目标点比较远时，梯度较小（趋于0）；当距离目标点较近时，梯度也较小（趋于0），不利于优化；","link":"/daka/posts/c4556752/"},{"title":"均方差损失和交叉熵损失 - 001","text":"2022-06-04 打卡； 补充说明： 这里上述推导过程中使用了sigmoid函数作为分类器，也就意味着这里假设任务为二分类任务；如果假设这里是多分类任务，那么就需要使用softmax函数作为分类器，猜测最终推导出的结论应该是类似的，还没有推导，之后补上； 上述推导说明了MSE在标签值为离散时，且输出值后面接了sigmoid/softmax时效果不好，但是当输出值为连续的，且不接sigmoid/softmax时，MSE的效果就相对比较好了；","link":"/daka/posts/2a5b067e/"},{"title":"论文：Layer Normalization（第1天）","text":"2022-05-16 打卡；","link":"/daka/posts/78936358/"},{"title":"论文：Layer Normalization（第2天）","text":"2022-05-17 打卡；","link":"/daka/posts/497b79c5/"},{"title":"论文：Switch Transformers（第1天）","text":"","link":"/daka/posts/3fb7c9fe/"},{"title":"论文：Switch Transformers（第2天）","text":"2022-05-09 打卡；","link":"/daka/posts/e5fd363/"},{"title":"论文：Switch Transformers（第3天）","text":"2022-05-10 打卡；","link":"/daka/posts/a828d8d7/"},{"title":"论文：Switch Transformers（第5天）","text":"2022-05-12 打卡； 行文目录 switch transformer 模型结构； switch transformer 的训练tricks（为了获取高指标方面的tricks）； 高效训练，并行训练（为了提高训练速度方面的tricks）： 数据并行； 模型并行； 专家并行； 实验结果： 预训练阶段的实验结果； 下游任务阶段的实验结果：包括微调效果、蒸馏效果、多语言上的效果； Switch Transformer 模型结构","link":"/daka/posts/cbf8eded/"},{"title":"回溯算法(DFS) - 001","text":"2022-05-13 打卡；","link":"/daka/posts/876b2150/"},{"title":"回溯算法(DFS) - 004","text":"2022-05-21 打卡；","link":"/daka/posts/f701d5df/"},{"title":"回溯算法(DFS) - 002","text":"2022-05-19 打卡； 题目题目为：给你输入一个数组 nums 和一个正整数 k，请你判断 nums 是否能够被平分为元素和相同的 k 个子集。 还未完全理解，所以只有草稿。并且该草稿中需要证明的第2部分是错误的。 一个递归说明以下两层for循环的代码输出结果为：45 12345678910111213import numpy as npcounter = 0def for_loop(start, end): global counter for i in np.arange(start, end, 1): for j in np.arange(i + 1, end, 1): counter += 1for_loop(0, 10)print(counter) 以下一个for循环加一个递归的代码输出结果为：1023可以看出这个代码其遍历的情况远多于for循环，所以性能远差于两层for循环； 12345678910111213import numpy as npcounter = 0def backtrack(start, end): global counter for i in np.arange(start, end, 1): counter += 1 backtrack(i + 1, end)backtrack(0, 10)print(counter)","link":"/daka/posts/1e6270ea/"},{"title":"回溯算法(DFS) - 005","text":"2022-05-23 打卡；","link":"/daka/posts/8006e549/"},{"title":"回溯算法(DFS) - 006","text":"2022-05-24 打卡；","link":"/daka/posts/190fb4f3/"},{"title":"论文：Switch Transformers（第4天）","text":"2022-05-11 打卡；","link":"/daka/posts/6d8fe659/"},{"title":"回溯算法(DFS) - 003","text":"2022-05-20 打卡；","link":"/daka/posts/6965407c/"},{"title":"论文：Switch Transformers（第4天-晚间）","text":"2022-05-11 打卡；","link":"/daka/posts/324ab1f/"},{"title":"回溯算法(DFS) - 007","text":"2022-05-25 打卡；","link":"/daka/posts/6e088465/"},{"title":"回溯算法(DFS) - 008","text":"2022-05-27 打卡；","link":"/daka/posts/feb799f4/"},{"title":"BFS - 001","text":"2022-05-26 打卡； 1、Python中队列的常用方法 入队； 出队； 查看队列长度； 判断队列是否为空； 代码示例： 1234567891011121314151617181920212223from queue import Queueq = Queue()# 入队q.put(&quot;aaa&quot;)q.put(&quot;bbb&quot;)# 查看队列长度q_size = q.qsize()print(&quot;the queue size: &quot;, q_size)# 判断队列是否为空print(&quot;check queue is empty: &quot;, q.empty())# 出队x = q.get()print(&quot;the first element in queue: &quot;, x)x = q.get()print(&quot;the second element in queue: &quot;, x)# 判断队列是否为空print(&quot;check queue is empty: &quot;, q.empty()) 输出结果如下： 12345the queue size: 2check queue is empty: Falsethe first element in queue: aaathe second element in queue: bbbcheck queue is empty: True 2、刷题2.1 二叉树最小深度 2.2 打开转盘锁题目： 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from queue import Queuedef up_operate(passwd ,i): &quot;&quot;&quot; 将密码中第i个位置的数字加1 &quot;&quot;&quot; if int(passwd[i]) == 9: return f&quot;{passwd[:i]}0{passwd[i+1:]}&quot; else: return f&quot;{passwd[:i]}{int(passwd[i])+1}{passwd[i+1:]}&quot;def down_operate(passwd, i): &quot;&quot;&quot; 将密码中第i个位置的数字减1 &quot;&quot;&quot; if int(passwd[i]) == 0: return f&quot;{passwd[:i]}9{passwd[i+1:]}&quot; else: return f&quot;{passwd[:i]}{int(passwd[i])-1}{passwd[i+1:]}&quot;def solve(deadends, target): start_passwd = &quot;0000&quot; if target == start_passwd: return 0 q, visited, depth, deadends = Queue(), {start_passwd, }, 1, set(deadends) q.put(start_passwd) while not q.empty(): q_size = q.qsize() for _ in range(q_size): passwd = q.get() # 遍历8种情况，类似8叉树 for i in range(len(passwd)): passwd_up = up_operate(passwd, i) if passwd_up not in deadends: if target == passwd_up: return depth if passwd_up not in visited: q.put(passwd_up) visited.add(passwd_up) passwd_down = down_operate(passwd, i) if passwd_down not in deadends: if target == passwd_down: return depth if passwd_down not in visited: q.put(passwd_down) visited.add(passwd_down) depth += 1 return -1if __name__ == &quot;__main__&quot;: deadends = [&quot;0201&quot;, &quot;0101&quot;, &quot;0102&quot;, &quot;1212&quot;, &quot;2002&quot;] target = &quot;0202&quot; # deadends = [&quot;8888&quot;] # target = &quot;0009&quot; # deadends = [&quot;8887&quot;, &quot;8889&quot;, &quot;7888&quot;, &quot;9888&quot;, &quot;8788&quot;, &quot;8988&quot;, &quot;8878&quot;, &quot;8898&quot;] # target = &quot;8888&quot; print(solve(deadends, target))","link":"/daka/posts/df9e958b/"},{"title":"二叉搜索树 - 003","text":"2022-06-03 打卡； 不同的二叉搜索树问题 思路遍历每个数字作为根节点，然后该数字左边比它小的数字作为左子树，该数字右边比它大的数字作为右子树，这样递归进行； 暴力代码123456789101112131415class Solution: def numTrees(self, n: int) -&gt; int: return self.recursive(0, n) def recursive(self, left, right): if left &gt;= right - 1: return 1 sum = 0 for i in range(left, right): left_num = self.recursive(left, i) right_num = self.recursive(i + 1, right) sum += (left_num * right_num) return sum 动态规划代码只需要在暴力代码的基础上增加一个备忘即可； 12345678910111213141516171819202122class Solution: def __init__(self): self.cache = {} def numTrees(self, n: int) -&gt; int: return self.recursive(0, n) def recursive(self, left, right): if left &gt;= right - 1: return 1 if (left, right) in self.cache: return self.cache[(left, right)] sum = 0 for i in range(left, right): left_num = self.recursive(left, i) right_num = self.recursive(i + 1, right) sum += (left_num * right_num) self.cache[(left, right)] = sum return sum 不同的二叉搜索树 II问题 思路这个题目与上一题目大同小异，只不过上一题目只需要计数，这个题目改为将中间结果都存储下来； 暴力代码123456789101112131415161718192021222324252627282930# Definition for a binary tree node.# class TreeNode:# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution: def generateTrees(self, n: int) -&gt; List[TreeNode]: return self.recursive(0, n) def recursive(self, left, right): if left &gt;= right: return [None] if left == right - 1: return [TreeNode(val=left + 1)] tree_list = [] for i in range(left, right): left_tree_list = self.recursive(left, i) right_tree_list = self.recursive(i + 1, right) for left_tree in left_tree_list: for right_tree in right_tree_list: root = TreeNode(val=i + 1) root.left = left_tree root.right = right_tree tree_list.append(root) return tree_list 动态规划代码只需要在暴力代码的基础上增加一个备忘即可； 12345678910111213141516171819202122232425262728293031323334353637# Definition for a binary tree node.# class TreeNode:# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution: def __init__(self): self.cache = {} def generateTrees(self, n: int) -&gt; List[TreeNode]: return self.recursive(0, n) def recursive(self, left, right): if left &gt;= right: return [None] if left == right - 1: return [TreeNode(val=left + 1)] if (left, right) in self.cache: return self.cache[(left, right)] tree_list = [] for i in range(left, right): left_tree_list = self.recursive(left, i) right_tree_list = self.recursive(i + 1, right) for left_tree in left_tree_list: for right_tree in right_tree_list: root = TreeNode(val=i + 1) root.left = left_tree root.right = right_tree tree_list.append(root) self.cache[(left, right)] = tree_list return tree_list","link":"/daka/posts/a5f897a5/"},{"title":"二叉搜索树 - 001","text":"2022-06-01 打卡；","link":"/daka/posts/4bf6f689/"},{"title":"二叉搜索树 - 002","text":"2022-06-02 打卡； 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Node: def __int__(self, value): self.left = None self.right = None self.val = valueclass BST: def check(self, root): &quot;&quot;&quot; 判断二叉搜索树的合法性 &quot;&quot;&quot; flag,_, _ = self.check_recursive(root) return flag def check_recursive(self, root): if root is None: return True, None, None left_flag, left_min, left_max = self.check_recursive(root.left) right_flag, right_min, right_max = self.check_recursive(root.right) if not left_flag or not right_flag: return False, None, None if left_max is not None and left_max.val &gt;= root.val: return False, None, None if right_min is not None and right_min.val &lt;= root.val: return False, None, None min = left_min if left_min is not None else root max = right_max if right_max is not None else root return True, min, max def search(self, root, target): &quot;&quot;&quot; 搜索 &quot;&quot;&quot; if root is None: return None if root.val == target: return root if root.val &lt; target: return self.search(root.left, target) else: return self.search(root.right, target) def insert(self, root, val): &quot;&quot;&quot; 插入 &quot;&quot;&quot; if root is None: return Node(value=val) p_node, node = None, root while node is not None: p_node = node if node.val &lt; val: node = node.right else: node = node.left if p_node.val &lt; val: p_node.right = Node(value=val) else: p_node.left = Node(value=val)","link":"/daka/posts/d2ffa733/"},{"title":"二叉树 - 003","text":"2022-05-30 打卡；","link":"/daka/posts/cf87383a/"},{"title":"二叉树 - 004","text":"2022-05-31 打卡；","link":"/daka/posts/51e3ad99/"},{"title":"二叉树 - 001","text":"2022-05-28 打卡；","link":"/daka/posts/21895916/"},{"title":"二叉树 - 002","text":"2022-05-29 打卡；","link":"/daka/posts/b88008ac/"}],"tags":[{"name":"NLP算法","slug":"NLP算法","link":"/daka/tags/NLP%E7%AE%97%E6%B3%95/"},{"name":"BN","slug":"BN","link":"/daka/tags/BN/"},{"name":"手写","slug":"手写","link":"/daka/tags/%E6%89%8B%E5%86%99/"},{"name":"LN","slug":"LN","link":"/daka/tags/LN/"},{"name":"打卡规则","slug":"打卡规则","link":"/daka/tags/%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/"},{"name":"mk书写","slug":"mk书写","link":"/daka/tags/mk%E4%B9%A6%E5%86%99/"},{"name":"KL散度","slug":"KL散度","link":"/daka/tags/KL%E6%95%A3%E5%BA%A6/"},{"name":"线性回归","slug":"线性回归","link":"/daka/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"name":"L1正则","slug":"L1正则","link":"/daka/tags/L1%E6%AD%A3%E5%88%99/"},{"name":"L2正则","slug":"L2正则","link":"/daka/tags/L2%E6%AD%A3%E5%88%99/"},{"name":"Jacobi矩阵","slug":"Jacobi矩阵","link":"/daka/tags/Jacobi%E7%9F%A9%E9%98%B5/"},{"name":"常用距离","slug":"常用距离","link":"/daka/tags/%E5%B8%B8%E7%94%A8%E8%B7%9D%E7%A6%BB/"},{"name":"均方差损失","slug":"均方差损失","link":"/daka/tags/%E5%9D%87%E6%96%B9%E5%B7%AE%E6%8D%9F%E5%A4%B1/"},{"name":"交叉熵损失","slug":"交叉熵损失","link":"/daka/tags/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1/"},{"name":"预训练模型","slug":"预训练模型","link":"/daka/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"Sparse Model","slug":"Sparse-Model","link":"/daka/tags/Sparse-Model/"},{"name":"MoE","slug":"MoE","link":"/daka/tags/MoE/"},{"name":"回溯算法","slug":"回溯算法","link":"/daka/tags/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"},{"name":"DFS","slug":"DFS","link":"/daka/tags/DFS/"},{"name":"BFS","slug":"BFS","link":"/daka/tags/BFS/"},{"name":"二叉搜索树","slug":"二叉搜索树","link":"/daka/tags/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"name":"二叉树","slug":"二叉树","link":"/daka/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"categories":[{"name":"NLP算法","slug":"NLP算法","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/"},{"name":"0_打卡规则","slug":"0-打卡规则","link":"/daka/categories/0-%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/"},{"name":"NLP基础","slug":"NLP算法/NLP基础","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/NLP%E5%9F%BA%E7%A1%80/"},{"name":"损失函数","slug":"NLP算法/损失函数","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"name":"预训练模型","slug":"NLP算法/预训练模型","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"刷题","slug":"刷题","link":"/daka/categories/%E5%88%B7%E9%A2%98/"},{"name":"树结构","slug":"刷题/树结构","link":"/daka/categories/%E5%88%B7%E9%A2%98/%E6%A0%91%E7%BB%93%E6%9E%84/"},{"name":"回溯算法","slug":"刷题/回溯算法","link":"/daka/categories/%E5%88%B7%E9%A2%98/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"},{"name":"BFS","slug":"刷题/BFS","link":"/daka/categories/%E5%88%B7%E9%A2%98/BFS/"}],"pages":[]}