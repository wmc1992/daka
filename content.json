{"pages":[],"posts":[{"title":"Batch Normalize","text":"2022-05-14 打卡； Referencehttps://zhuanlan.zhihu.com/p/34879333","link":"/daka/2022/05/14/nlp_basis/batch_normalize/"},{"title":"打卡规则","text":"","link":"/daka/2022/05/09/%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/"},{"title":"Batch Normalize","text":"2022-05-15 打卡； 1、BN具体的操作步骤其操作步骤分为两部分： 每个 mini-batch 内的特征通过减去 $\\mu$ 除以 $\\sigma$ 的方式归一化到标准正态分布； 对经过上一步骤的输出再过一个线性变换； 以上是文字形式的说明，下面是公式形式。 输入：一个 mini-batch 内的数据为 ${x_1, x_2, …, x_m}$，$m$ 是 mini-batch-size；记 $x_i^{(j)}$ 为该 mini-batch 中第 $i$ 条数据的第 $j$ 个特征；$\\gamma$ 和 $\\beta$ 为可学习的参数； 输出：$y_i^{(j)}$ 为该 mini-batch 中第 $i$ 条数据的第 $j$ 个特征经过BN后的结果。 公式： $\\mu_{B}^{(j)} = \\frac{1}{m} \\sum_{i=1}^{m} x_{i}^{(j)} \\quad \\text{// 该 mini-batch 中第j个特征的均值}$ $ \\sigma_{B}^{(j)2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_i^{(j)} - \\mu_B^{(j)})^2 \\quad \\text{// 该 mini-batch 中第j个特征的方差}$ $\\hat{x}_i^{(j)} = \\frac{x_i^{(j)} - \\mu_B^{(j)}}{\\sqrt{\\sigma_B^{(j)2} + \\epsilon}} $ $y_i^{(j)} = \\gamma \\hat{x_i^{(j)}} + \\beta \\quad \\text{// 经过BN后的最终输出结果}$ 2、BN是为了解决什么问题2.1 ICS问题（Internal Covariate Shift）在深度模型中，随着训练的进行，网络中的参数随着梯度下降在不断更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活，这些微弱变化随着网络层数的加深而被加大；另一方面，参数的变化导致每一层的输入分布会发生变化，进而上层的网络需要不停的去适应这些分布的变化，使的模型训练变得困难。这一现象就是ICS问题。 2.2 ICS会导致的问题 由于下层网络的输出数据分布在不断变化，上层网络就需要不停调整来适应这个变化，这会导致网络学习速度的降低； 随着模型训练的进行，模型的参数 $W$ 会变大，则每层的输出 $ \\text{Output} = W * \\text{Input} + b $ 也会变大，当使用饱和激活函数时，就容易陷入梯度饱和区，此时的梯度就会很小甚至于接近0. 3、在BN出现前，是如何解决上述问题的3.1 白化操作ICS产生的原因是由于参数的更新带来的网络中每一层输入值分布的改变，并且随着网络的加深而越严重。缓解ICS最直观的想法就是：固定每层网络输入值的分布来缓解ICS问题。 白化操作：经过变化操作之后会具有如下两个性质： 经过白化后，其特征之间的相关性较低； 经过白化后，所有特征具有相同的方差； 在BN出现之前，主要是利用白化操作来缓解ICS问题：主要是PCA白化和ZCA白化。 3.2 白化操作的缺点 计算成本太高； 白化操作实际上是改变了网络每一层的分布，所以其也改变了网络的表达能力； Referencehttps://zhuanlan.zhihu.com/p/34879333 https://www.zhihu.com/question/395811291 https://www.zhihu.com/question/487766088","link":"/daka/2022/05/15/nlp_basis/batch_normalize_markdown/"},{"title":"Switch Transformers（第2天）","text":"2022-05-09 打卡；","link":"/daka/2022/05/09/paper/switch_transformers/switch_transformers_%E7%AC%AC2%E5%A4%A9/"},{"title":"刷题打卡 - 回溯算法","text":"2022-05-13 打卡；","link":"/daka/2022/05/13/shuati/shuati_%E7%AC%AC001%E5%A4%A9/"},{"title":"Switch Transformers（第3天）","text":"2022-05-10 打卡；","link":"/daka/2022/05/10/paper/switch_transformers/switch_transformers_%E7%AC%AC3%E5%A4%A9/"},{"title":"Switch Transformers（第4天-晚间）","text":"2022-05-11 打卡；","link":"/daka/2022/05/11/paper/switch_transformers/switch_transformers_%E7%AC%AC4%E5%A4%A9_%E6%99%9A%E9%97%B4/"},{"title":"Switch Transformers（第4天）","text":"2022-05-11 打卡；","link":"/daka/2022/05/11/paper/switch_transformers/switch_transformers_%E7%AC%AC4%E5%A4%A9/"},{"title":"Switch Transformers（第5天）","text":"2022-05-12 打卡； 行文目录 switch transformer 模型结构； switch transformer 的训练tricks（为了获取高指标方面的tricks）； 高效训练，并行训练（为了提高训练速度方面的tricks）： 数据并行； 模型并行； 专家并行； 实验结果： 预训练阶段的实验结果； 下游任务阶段的实验结果：包括微调效果、蒸馏效果、多语言上的效果； Switch Transformer 模型结构","link":"/daka/2022/05/12/paper/switch_transformers/switch_transformers_%E7%AC%AC5%E5%A4%A9/"},{"title":"Layer Normalization（第1天）","text":"2022-05-16 打卡；","link":"/daka/2022/05/16/paper/layer_normalization/layer_normalization_%E7%AC%AC1%E5%A4%A9/"},{"title":"Switch Transformers（第1天）","text":"","link":"/daka/2022/05/09/paper/switch_transformers/switch_transformers_%E7%AC%AC1%E5%A4%A9/"}],"tags":[{"name":"NLP算法","slug":"NLP算法","link":"/daka/tags/NLP%E7%AE%97%E6%B3%95/"},{"name":"BN","slug":"BN","link":"/daka/tags/BN/"},{"name":"打卡规则","slug":"打卡规则","link":"/daka/tags/%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/"},{"name":"预训练模型","slug":"预训练模型","link":"/daka/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"Sparse Model","slug":"Sparse-Model","link":"/daka/tags/Sparse-Model/"},{"name":"MoE","slug":"MoE","link":"/daka/tags/MoE/"},{"name":"回溯算法","slug":"回溯算法","link":"/daka/tags/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"},{"name":"LN","slug":"LN","link":"/daka/tags/LN/"}],"categories":[{"name":"NLP算法","slug":"NLP算法","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/"},{"name":"0_打卡规则","slug":"0-打卡规则","link":"/daka/categories/0-%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/"},{"name":"预训练模型","slug":"NLP算法/预训练模型","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"NLP基础","slug":"NLP算法/NLP基础","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/NLP%E5%9F%BA%E7%A1%80/"},{"name":"刷题","slug":"刷题","link":"/daka/categories/%E5%88%B7%E9%A2%98/"},{"name":"回溯算法","slug":"刷题/回溯算法","link":"/daka/categories/%E5%88%B7%E9%A2%98/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"}]}