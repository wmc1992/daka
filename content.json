{"pages":[],"posts":[{"title":"Layer Normalize - markdown","text":"2022-05-22 打卡； 1、LN的具体操作步骤其操作步骤可分为三部分： 求每条数据各特征之间的均值和标准差； 每条数据的每个特征减去各自数据的均值，除上各自数据的标准差； 对经过上一步骤的输出再经过一个线性变换； 以上是文字形式的说明，以下是公式形式。 输入： 一个 mini-batch 的数据在某层网络的输出为 $\\{\\alpha_1, \\alpha_2, …, \\alpha_m\\}$，其中 $m$ 为batch size； 记 $\\alpha_i^{(j)}$ 为该mini-batch中第 $i$ 条数据的第 $j$ 个特征； $T$ 为每条数据的特征数，每条数据的特征数不一定相同； $g$ 和 $b$ 为可学习参数； 按照上述定义，某层网络的输出的shape为 $[m, T]$，$m$ 为batch-size，$T$ 为每条数据的特征数量； 输出： $y_i^{(j)}$ 为该 mini-batch 中第 $i$ 条数据第 $j$ 个特征经过LN之后的输出； 公式： 第 $i$ 条数据各特征的均值： $$\\mu_i = \\frac{1}{T} \\sum_{j=1}^{T} \\alpha_i^{(j)} $$ 第 $i$ 条数据各特征的方差： $$\\sigma_i^2 = \\frac{1}{T} \\sum_{j=1}^{T} (\\alpha_i^{(j)} - \\mu_i)^2$$ 减去均值，除上标准化，$\\epsilon$ 用于避免除数为0： $$\\hat{\\alpha_i^{(j)}} = \\frac{\\alpha_i^{(j)} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}} $$ 第 $i$ 条数据第 $j$ 个特征经过LN后的结果： $$y_i^{(j)} = g \\hat{\\alpha_i^{(j)}} + b $$ 2、LN 为了解决什么问题 深度模型训练时所需要的计算资源非常大，想要减少训练所需时间的一个方法是：normalize the activities of neurons 增加训练过程的稳定性； 3、LN 出现之前是如何解决上述问题的LN 出现之前通过 BN 解决上述问题； BN的优点： 可以解决 “convariate shift” 问题，缩短了模型训练所需的时间； 能够使饱和激活函数的输入落在非饱和区，增加了训练的稳定性； BN的缺点： 当 batch size 特别小时，表现不好； 当每条数据的长度不一致时，比如文本数据，效果不好； 在 RNN 网络中，表现不好； 4、LN 的优势Normalization 的作用：降低了对参数初始化的需求，允许使用更大的学习率，有一定的正则化作用可抗过拟合，使训练更加稳定。 假设某一层输出的中间结果为 $[m, T]$，$m$ 为batch-size，$T$ 为每条数据的特征数量，那么： BN 是对 $m$ 这个维度做归一化； LN 是对 $T$ 这个维度做归一化； 优势（以下都有待考证）： 在 RNN 网络中，表现较好； 在 batch size 较小的网络中，表现较好； LN 抹杀了不同样本间的大小关系，保留了同一个样本内部的特征之间的大小关系，这对于时间序列任务或NLP任务来说非常重要；","link":"/daka/2022/05/22/nlp_basis/layer_normalize_markdown/"},{"title":"Layer Normalize - 手写","text":"2022-05-18 打卡；","link":"/daka/2022/05/18/nlp_basis/layer_normalize/"},{"title":"Batch Normalize - markdown","text":"2022-05-15 打卡； 1、BN具体的操作步骤其操作步骤分为两部分： 每个 mini-batch 内的特征通过减去 $\\mu$ 除以 $\\sigma$ 的方式归一化到标准正态分布； 对经过上一步骤的输出再过一个线性变换； 以上是文字形式的说明，下面是公式形式。 输入： 一个 mini-batch 内的数据为 $ \\{x_1, x_2, …, x_m \\}$，其中 $m$ 是 mini-batch-size； 记 $x_i^{(j)}$ 为该 mini-batch 中第 $i$ 条数据的第 $j$ 个特征； $\\gamma$ 和 $\\beta$ 为可学习的参数； 输出： $y_i^{(j)}$ 为该 mini-batch 中第 $i$ 条数据的第 $j$ 个特征经过BN后的结果。 公式： 该 mini-batch 中第j个特征的均值： $$\\mu_{B}^{(j)} = \\frac{1}{m} \\sum_{i=1}^{m} x_{i}^{(j)}$$ 该 mini-batch 中第j个特征的方差： $$ \\sigma_{B}^{(j)2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_i^{(j)} - \\mu_B^{(j)})^2$$ 减去均值，除上标准化，$\\epsilon$ 用于避免除数为0： $$\\hat{x}_i^{(j)} = \\frac{x_i^{(j)} - \\mu_B^{(j)}}{\\sqrt{\\sigma_B^{(j)2} + \\epsilon}} $$ 经过BN后的最终输出结果： $$y_i^{(j)} = \\gamma \\hat{x_i^{(j)}} + \\beta $$ 2、BN是为了解决什么问题2.1 ICS问题（Internal Covariate Shift）在深度模型中，随着训练的进行，网络中的参数随着梯度下降在不断更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活，这些微弱变化随着网络层数的加深而被加大；另一方面，参数的变化导致每一层的输入分布会发生变化，进而上层的网络需要不停的去适应这些分布的变化，使的模型训练变得困难。这一现象就是ICS问题。 2.2 ICS会导致的问题 由于下层网络的输出数据分布在不断变化，上层网络就需要不停调整来适应这个变化，这会导致网络学习速度的降低； 随着模型训练的进行，模型的参数 $W$ 会变大，则每层的输出 $ \\text{Output} = W * \\text{Input} + b $ 也会变大，当使用饱和激活函数时，就容易陷入梯度饱和区，此时的梯度就会很小甚至于接近0. 3、在BN出现前，是如何解决上述问题的3.1 白化操作ICS产生的原因是由于参数的更新带来的网络中每一层输入值分布的改变，并且随着网络的加深而越严重。缓解ICS最直观的想法就是：固定每层网络输入值的分布来缓解ICS问题。 白化操作：经过变化操作之后会具有如下两个性质： 经过白化后，其特征之间的相关性较低； 经过白化后，所有特征具有相同的方差； 在BN出现之前，主要是利用白化操作来缓解ICS问题：主要是PCA白化和ZCA白化。 3.2 白化操作的缺点 计算成本太高； 白化操作实际上是改变了网络每一层的分布，所以其也改变了网络的表达能力； 4、BN的作用与优势 BN将网络每一层输入数据的分布固定在一定的范围内，使上层网络不需频繁的去适应底层网络，加速了模型的学习速度； BN能够使每层网络的输入落在饱和激活函数的非饱和区，缓解梯度消失问题； BN具有一定的正则化效果，由于每个 mini-batch 的均值与方差不完全相同，相当于增加了随机噪声，使其具有一定的正则化效果； 由于BN操作中可学习参数 $\\gamma$ 和 $\\beta$ 的存在，BN缓解了上述所说的白化操作的第2个缺点； 经过BN操作之后，模型权重的缩放（比如 $W =&gt; aW$ ）会被“抹去”，这使模型对网络中的参数不再过分敏感，训练时更加稳定。 Referencehttps://zhuanlan.zhihu.com/p/34879333 https://www.zhihu.com/question/395811291 https://www.zhihu.com/question/487766088","link":"/daka/2022/05/15/nlp_basis/batch_normalize_markdown/"},{"title":"刷题打卡002 - 回溯算法","text":"2022-05-19 打卡； 题目题目为：给你输入一个数组 nums 和一个正整数 k，请你判断 nums 是否能够被平分为元素和相同的 k 个子集。 还未完全理解，所以只有草稿。并且该草稿中需要证明的第2部分是错误的。 一个递归说明以下两层for循环的代码输出结果为：45 12345678910111213import numpy as npcounter = 0def for_loop(start, end): global counter for i in np.arange(start, end, 1): for j in np.arange(i + 1, end, 1): counter += 1for_loop(0, 10)print(counter) 以下一个for循环加一个递归的代码输出结果为：1023可以看出这个代码其遍历的情况远多于for循环，所以性能远差于两层for循环； 12345678910111213import numpy as npcounter = 0def backtrack(start, end): global counter for i in np.arange(start, end, 1): counter += 1 backtrack(i + 1, end)backtrack(0, 10)print(counter)","link":"/daka/2022/05/19/shuati/shuati_%E7%AC%AC002%E5%A4%A9/"},{"title":"刷题打卡004 - 回溯算法","text":"2022-05-21 打卡；","link":"/daka/2022/05/21/shuati/shuati_%E7%AC%AC004%E5%A4%A9/"},{"title":"Batch Normalize - 手写","text":"2022-05-14 打卡； Referencehttps://zhuanlan.zhihu.com/p/34879333","link":"/daka/2022/05/14/nlp_basis/batch_normalize/"},{"title":"刷题打卡003 - 回溯算法","text":"2022-05-20 打卡；","link":"/daka/2022/05/20/shuati/shuati_%E7%AC%AC003%E5%A4%A9/"},{"title":"刷题打卡001 - 回溯算法","text":"2022-05-13 打卡；","link":"/daka/2022/05/13/shuati/shuati_%E7%AC%AC001%E5%A4%A9/"},{"title":"刷题打卡006 - 回溯算法","text":"2022-05-24 打卡；","link":"/daka/2022/05/24/shuati/shuati_%E7%AC%AC006%E5%A4%A9/"},{"title":"刷题打卡007 - 回溯算法","text":"2022-05-25 打卡；","link":"/daka/2022/05/25/shuati/shuati_%E7%AC%AC007%E5%A4%A9/"},{"title":"刷题打卡008 - BFS","text":"2022-05-26 打卡； 1、Python中队列的常用方法 入队； 出队； 查看队列长度； 判断队列是否为空； 代码示例： 1234567891011121314151617181920212223from queue import Queueq = Queue()# 入队q.put(&quot;aaa&quot;)q.put(&quot;bbb&quot;)# 查看队列长度q_size = q.qsize()print(&quot;the queue size: &quot;, q_size)# 判断队列是否为空print(&quot;check queue is empty: &quot;, q.empty())# 出队x = q.get()print(&quot;the first element in queue: &quot;, x)x = q.get()print(&quot;the second element in queue: &quot;, x)# 判断队列是否为空print(&quot;check queue is empty: &quot;, q.empty()) 输出结果如下： 12345the queue size: 2check queue is empty: Falsethe first element in queue: aaathe second element in queue: bbbcheck queue is empty: True 2、刷题2.1 二叉树最小深度 2.2 打开转盘锁题目： 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from queue import Queuedef up_operate(passwd ,i): &quot;&quot;&quot; 将密码中第i个位置的数字加1 &quot;&quot;&quot; if int(passwd[i]) == 9: return f&quot;{passwd[:i]}0{passwd[i+1:]}&quot; else: return f&quot;{passwd[:i]}{int(passwd[i])+1}{passwd[i+1:]}&quot;def down_operate(passwd, i): &quot;&quot;&quot; 将密码中第i个位置的数字减1 &quot;&quot;&quot; if int(passwd[i]) == 0: return f&quot;{passwd[:i]}9{passwd[i+1:]}&quot; else: return f&quot;{passwd[:i]}{int(passwd[i])-1}{passwd[i+1:]}&quot;def solve(deadends, target): start_passwd = &quot;0000&quot; if target == start_passwd: return 0 q, visited, depth, deadends = Queue(), {start_passwd, }, 1, set(deadends) q.put(start_passwd) while not q.empty(): q_size = q.qsize() for _ in range(q_size): passwd = q.get() # 遍历8种情况，类似8叉树 for i in range(len(passwd)): passwd_up = up_operate(passwd, i) if passwd_up not in deadends: if target == passwd_up: return depth if passwd_up not in visited: q.put(passwd_up) visited.add(passwd_up) passwd_down = down_operate(passwd, i) if passwd_down not in deadends: if target == passwd_down: return depth if passwd_down not in visited: q.put(passwd_down) visited.add(passwd_down) depth += 1 return -1if __name__ == &quot;__main__&quot;: deadends = [&quot;0201&quot;, &quot;0101&quot;, &quot;0102&quot;, &quot;1212&quot;, &quot;2002&quot;] target = &quot;0202&quot; # deadends = [&quot;8888&quot;] # target = &quot;0009&quot; # deadends = [&quot;8887&quot;, &quot;8889&quot;, &quot;7888&quot;, &quot;9888&quot;, &quot;8788&quot;, &quot;8988&quot;, &quot;8878&quot;, &quot;8898&quot;] # target = &quot;8888&quot; print(solve(deadends, target))","link":"/daka/2022/05/26/shuati/shuati_%E7%AC%AC008%E5%A4%A9/"},{"title":"打卡规则","text":"","link":"/daka/2022/05/09/%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/"},{"title":"刷题打卡005 - 回溯算法","text":"2022-05-23 打卡；","link":"/daka/2022/05/23/shuati/shuati_%E7%AC%AC005%E5%A4%A9/"},{"title":"论文：Layer Normalization（第2天）","text":"2022-05-17 打卡；","link":"/daka/2022/05/17/paper/layer_normalization/layer_normalization_%E7%AC%AC2%E5%A4%A9/"},{"title":"论文：Switch Transformers（第2天）","text":"2022-05-09 打卡；","link":"/daka/2022/05/09/paper/switch_transformers/switch_transformers_%E7%AC%AC2%E5%A4%A9/"},{"title":"论文：Switch Transformers（第1天）","text":"","link":"/daka/2022/05/09/paper/switch_transformers/switch_transformers_%E7%AC%AC1%E5%A4%A9/"},{"title":"论文：Layer Normalization（第1天）","text":"2022-05-16 打卡；","link":"/daka/2022/05/16/paper/layer_normalization/layer_normalization_%E7%AC%AC1%E5%A4%A9/"},{"title":"论文：Switch Transformers（第4天）","text":"2022-05-11 打卡；","link":"/daka/2022/05/11/paper/switch_transformers/switch_transformers_%E7%AC%AC4%E5%A4%A9/"},{"title":"论文：Switch Transformers（第4天-晚间）","text":"2022-05-11 打卡；","link":"/daka/2022/05/11/paper/switch_transformers/switch_transformers_%E7%AC%AC4%E5%A4%A9_%E6%99%9A%E9%97%B4/"},{"title":"论文：Switch Transformers（第3天）","text":"2022-05-10 打卡；","link":"/daka/2022/05/10/paper/switch_transformers/switch_transformers_%E7%AC%AC3%E5%A4%A9/"},{"title":"论文：Switch Transformers（第5天）","text":"2022-05-12 打卡； 行文目录 switch transformer 模型结构； switch transformer 的训练tricks（为了获取高指标方面的tricks）； 高效训练，并行训练（为了提高训练速度方面的tricks）： 数据并行； 模型并行； 专家并行； 实验结果： 预训练阶段的实验结果； 下游任务阶段的实验结果：包括微调效果、蒸馏效果、多语言上的效果； Switch Transformer 模型结构","link":"/daka/2022/05/12/paper/switch_transformers/switch_transformers_%E7%AC%AC5%E5%A4%A9/"}],"tags":[{"name":"NLP算法","slug":"NLP算法","link":"/daka/tags/NLP%E7%AE%97%E6%B3%95/"},{"name":"LN","slug":"LN","link":"/daka/tags/LN/"},{"name":"mk书写","slug":"mk书写","link":"/daka/tags/mk%E4%B9%A6%E5%86%99/"},{"name":"手写","slug":"手写","link":"/daka/tags/%E6%89%8B%E5%86%99/"},{"name":"BN","slug":"BN","link":"/daka/tags/BN/"},{"name":"回溯算法","slug":"回溯算法","link":"/daka/tags/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"},{"name":"DFS","slug":"DFS","link":"/daka/tags/DFS/"},{"name":"BFS","slug":"BFS","link":"/daka/tags/BFS/"},{"name":"打卡规则","slug":"打卡规则","link":"/daka/tags/%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/"},{"name":"预训练模型","slug":"预训练模型","link":"/daka/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"Sparse Model","slug":"Sparse-Model","link":"/daka/tags/Sparse-Model/"},{"name":"MoE","slug":"MoE","link":"/daka/tags/MoE/"}],"categories":[{"name":"NLP算法","slug":"NLP算法","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/"},{"name":"刷题","slug":"刷题","link":"/daka/categories/%E5%88%B7%E9%A2%98/"},{"name":"NLP基础","slug":"NLP算法/NLP基础","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/NLP%E5%9F%BA%E7%A1%80/"},{"name":"回溯算法","slug":"刷题/回溯算法","link":"/daka/categories/%E5%88%B7%E9%A2%98/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"},{"name":"BFS","slug":"刷题/BFS","link":"/daka/categories/%E5%88%B7%E9%A2%98/BFS/"},{"name":"0_打卡规则","slug":"0-打卡规则","link":"/daka/categories/0-%E6%89%93%E5%8D%A1%E8%A7%84%E5%88%99/"},{"name":"预训练模型","slug":"NLP算法/预训练模型","link":"/daka/categories/NLP%E7%AE%97%E6%B3%95/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"}]}