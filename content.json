{"pages":[],"posts":[{"title":"","text":"hello world","link":"/2022/05/08/hello_world/"},{"title":"PyTorch中优化器的源码和显存分析","text":"本文按照论文 An overview of gradient descent optimization algorithms 中提到的优化器顺序和PyTorch中对这些优化器的实现，先了解了解源码，然后再分析一下在训练过程中不同的优化器的显存使用量。 一、SGD优化器torch.Tensor: 是PyTorch中用来做高维的张量运算的基本数据结构。这个类里面会存储数据，也包含了常见的矩阵相加相乘、张量相加相乘等常见运算。 在构造神经网络时，直接使用torch.Tensor这个基础数据结构作为网络中的每个单元是比较不方便的，比如PyTorch中支持网络中的各层自动求导，同时肯定也会有一些数据是中间的临时数据，并不需要求导，甚至于并不属于神经网络的一部分。如果直接使用torch.Tensor，那么哪些Tensor属于神经网络需要求导，哪些Tensor不属于神经网络不需要求导，并不易区分，所以有了torch.nn.Parameter。 torch.nn.Parameter: 该类是构成神经网络的基础单元。它与torch.Tensor的区别是，在模型中（比如torch.nn.Module）所有torch.nn.Parameter类型的属性都会被自动添加到模型的参数中，并可以通过函数Module.parameters()进行迭代。 二、Momentum2.1 公式与源码公式 Momentum SGD的优化公式为： $$ v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta) $$ $$ \\theta = \\theta - v_t$$ 其中 $\\gamma$ 和 $\\eta$ 是超参数，分别为动量和学习率。 源码 PyTorch中关于Momentum SGD的代码简化后如下，PyTorch中SGD源码： 1234567891011121314151617181920212223242526class SGD(Optimizer): def __init__(self, params, defaults): self.state = defaultdict(dict) # 用来缓存优化器状态 def step(self): for group in self.param_groups: momentum = group[&quot;momentum&quot;] # 动量 lr = group[&quot;lr&quot;] # 学习率 for p in group[&quot;params&quot;]: if p.grad is None: # 如果当前参数梯度为None，则不需要更新 continue d_p = p.grad # 梯度 # 由下面这段代码可看出只有第一次执行时需要开辟新缓存，之后都是在更新self.state中的值 state = self.state[p] if &quot;momentum_buffer&quot; not in state: buf = torch.clone(d_p).detach() else: buf = state[&quot;momentum_buffer&quot;] buf.mul_(momentum).add_(d_p) state[&quot;momentum_buffer&quot;] = buf d_p = buf p.add_(d_p, alpha=-lr) # 将（梯度 * -学习率）更新到权重参数上 代码中主要变量含义： self.state：相比于没有动量的SGD，其多了一个字典属性self.state。其key为模型中需要优化的每个参数，value为该参数历史的梯度累积（这里的累积不是指直接相加）。 momentum：超参数，动量，即公式中的 $\\gamma$； lr：超参数，学习率，即公式中的 $\\eta$； p.grad：本次反向传播时计算的梯度值，即公式中的 $\\nabla_{\\theta} J(\\theta)$； 代码中的主要涉及运算的代码行： 第22行代码对应的运算为：$ \\text{buf} = \\text{buf} \\times \\text{momentum} + \\text{d_p} $ 第26行代码对应的运算为：$ \\text{p} = \\text{p} - \\text{lr} \\times \\text{d_p} $ 另外，代码中的学习率的位置与原始公式中学习率的位置是有一点差别的。原始公式中是学习率只作用于本次的新求得的梯度，历史梯度的累积值是不会每次都乘以学习率。代码中是先把本次的新梯度与历史梯度的累积值相加，然后统一乘上学习率。根据代码写出的公式如下： $$ v_t = \\gamma v_{t-1} + \\nabla_{\\theta} J(\\theta) $$ $$ \\theta = \\theta - \\eta v_t$$ 关于代码中的实现对原公式做了修改这一点在PyTorch对文档中也有说明，比如 SGD 中的 NOTE 部分。 2.2 显存占用测试显存占用的代码如下： 1234567891011121314151617181920212223import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)print(&quot;初始化优化器:&quot;, torch.cuda.memory_allocated() - current_memory) # 打印增加的显存for batch_idx in range(3): # 模拟实际训练时有很多个批次 inputs = torch.tensor([1.0] * 2048).cuda() outputs = model(inputs) loss = torch.mean(outputs) loss.backward() current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存 optimizer.step() print(f&quot;第 {batch_idx + 1} 个batch:&quot;, torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出结果如下:# 初始化优化器: 0# 第 1 个batch: 8388608# 第 2 个batch: 0# 第 3 个batch: 0 可以看出只有第一次优化时增加了8M（8388608 = 8M）显存，第二、三次优化时没有增加显存。 模型的参数量为: $ 2048 * 1024 = 2097152 = 2 * 2^{20} = 2M $，从代码中可知属性self.state会给每个参数保存一个历史梯度的累积值，所以Momentum SGD中的参数量与模型的参数量是相同的，也是$2M$。每个浮点数使用4个字节，所以总共是$8M$显存。 只有第1个batch增加了显存，后面的batch都不增加显存的原因是：在第1个之前，属性self.state是空字典，在执行第一个batch的优化时，申请了和模型参数量相同的空间来存储历史梯度的累积值。而在后面的batch中都是对self.state中现有值的修改，不再申请新的显存了。 三、Nesterov accelerated gradient原始公式： $$ v_t = \\gamma v_{t-1} + \\nabla_{\\theta} J(\\theta - \\gamma v_{t-1}) $$ $$ \\theta = \\theta - \\eta v_t$$ PyTorch使用的公式： $$ v_t = \\gamma (\\gamma v_{t-1} + \\nabla_{\\theta} J(\\theta)) + \\nabla_{\\theta} J(\\theta) $$ $$ \\theta = \\theta - \\eta v_t$$ 四、Adagrad4.1 公式与源码公式 优化器Adagrad会给每个参数不同的学习率，其原始公式如下，公式中的 $\\frac{\\eta}{ \\sqrt{G_{t,ii} + \\epsilon } }$ 即为新学习率： $$ g_{t,i} = \\nabla_{\\theta_t} J(\\theta_{t,i}) $$ $$ \\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\eta}{ \\sqrt{G_{t,ii} + \\epsilon } } g_{t,i} $$ 在之前的公式中使用不带右下角标的 $\\theta$ 表示模型的参数，不带右下角标的 $\\theta$ 代表着模型中的所有参数；右下角标中的 $i$ 表示所有参数中的第 $i$ 个参数；右下角标中的 $t$ 表示在对参数的多次迭代过程中第 $t$ 次迭代时的参数； 公式中的 $g_{t,i}$ 表示在第 $t$ 次迭代时，对第 $i$ 个参数求得的梯度； 公式中的 $ G_{t,ii} $ 表示模型中第 $i$ 个参数，从第 $1$ 次迭代到第 $t$ 次迭代过程中所有梯度的和，即如下公式： $$ G_{t,ii} = \\sum^t_{\\text{step}=0} g_{\\text{step},i} $$ 源码 PyTorch中关于优化器Adagrad的代码简化后如下，PyTorch中Adagrad源码 ： 123456789101112131415161718192021222324class Adagrad(Optimizer): def __init__(self, initial_accumulator_value=0): self.state = defaultdict(dict) # 用来缓存优化器状态 for group in self.param_groups: for p in group[&quot;params&quot;]: state = self.state[p] state[&quot;sum&quot;] = torch.full_like(p, initial_accumulator_value) # 默认使用全0初始化 def step(self): for group in self.param_groups: lr = group[&quot;lr&quot;] eps = group[&quot;eps&quot;] for p in group[&quot;params&quot;]: if p.grad is None: continue state = self.state[p] grad, state_sum = p.grad, state[&quot;sum&quot;] state_sum.addcmul_(grad, grad, value=1) std = state_sum.sqrt().add_(eps) p.addcdiv_(grad, std, value=-lr) 代码中主要变量含义： self.state：缓存优化器中所有需要存储的中间状态。 lr：超参数，学习率，即公式中的 $\\eta$； eps：超参数，用于平滑防止除数为0，即公式中的 $\\epsilon$； p.grad：当前参数本次反向传播时计算的梯度值，即公式中的 $g_{t,i}$； state_sum：当前参数历史梯度值之和，即公式中的 $G_{t,ii}$； 代码中的主要涉及运算的代码行： 第23行代码对应的运算为：$ \\text{state_sum} = \\text{state_sum} + \\text{grad} \\times \\text{grad} \\times 1 $ 第24行代码对应的运算为：$ \\text{std} = \\sqrt{\\text{state_sum} + \\text{eps}} $ 第25行代码对应的运算为：$ \\text{p} = \\text{p} - \\text{lr} \\times \\frac{\\text{grad}}{\\text{std}} = \\text{p} - \\frac{\\text{lr}}{\\text{std}} \\times \\text{grad}$ 这里的 $\\frac{\\text{lr}}{\\text{std}}$ 就是公式中的新学习率 $\\frac{\\eta}{ \\sqrt{G_{t,ii} + \\epsilon } }$ 。 4.2 显存占用测试显存占用的代码如下： 1234567891011121314151617181920212223import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存optimizer = torch.optim.Adagrad(model.parameters(), lr=0.1)print(&quot;初始化优化器:&quot;, torch.cuda.memory_allocated() - current_memory) # 打印增加的显存for batch_idx in range(3): inputs = torch.tensor([1.0] * 2048).cuda() outputs = model(inputs) loss = torch.mean(outputs) loss.backward() current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存 optimizer.step() print(f&quot;第 {batch_idx} 个batch:&quot;, torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出如下:# 初始化优化器: 8388608# 第 0 个batch: 0# 第 1 个batch: 0# 第 2 个batch: 0 和带动量的SGD不同，带动量的SGD在初始化时 self.state 为空字典，在第一次迭代时申请了和模型参数量相同大小的显存。而在Adagrad中由上一步的简化后的源码可以看出，是在初始化时就已经申请了和模型参数量相同大小的显存，在迭代过程中是直接在 self.state 中的值上进行修改，不需要再额外申请显存。 表现到测试结果上就是，初始化优化器时显存增加了与模型参数等同的大小的显存，参数量为：$ 2048 * 1024 = 2097152 = 2 * 2^{20} = 2M $，每个浮点数4个字节，共计 $8388608 = 8M$ 显存。在迭代过程中不再申请新的显存。 五、Adadelta5.1 公式与源码5.2 显存占用Reference An overview of gradient descent optimization algorithms: https://arxiv.org/abs/1609.04747 路遥知马力-Momentum: https://zhuanlan.zhihu.com/p/21486826 比Momentum更快：揭开Nesterov Accelerated Gradient的真面目: https://zhuanlan.zhihu.com/p/22810533 PyTorch源码浅析: https://www.52coding.com.cn/2019/05/05/PyTorch0/","link":"/2022/05/09/NLP%E7%AE%97%E6%B3%95/PyTorch%E4%B8%AD%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%BA%90%E7%A0%81%E5%92%8C%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"},{"title":"Hello World-1","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-1/"},{"title":"MacOS在根目录下新建文件夹","text":"目前电脑的系统是 Big Sur 11.4 版本，在该版本的系统上操作的有效。 直接修改文件 /etc/synthetic.conf，即会在根目录下新增文件夹。比如想要在根目录下创建 data。执行如下命令： 1sudo vim /etc/synthetic.conf 然后在文件 /etc/synthetic.conf 添加如下一行： 1data /Users/xxx/data 上面这一行分三部分： 第一部分是要在根目录下创建的文件夹的名称，这里假设创建一个名为 data 的文件夹； 第二部分是一个Tab，注意这里不是空格； 第三部分是在根目录下创建的 data 目录软链向哪里，根目录下不会真正创建一个文件夹，而是一个软链接。 修改完之后保存，重启电脑，再看根目录： 12345(base) ➜ / lltotal 10... ...lrwxr-xr-x 1 root wheel 24B 3 10 14:44 data -&gt; /Users/xxx/data... ... 根目录下多出了一个 data 目录，软链到 /Users/xxx/data。","link":"/2022/05/09/%E5%85%B6%E4%BB%96/MacOS%E5%9C%A8%E6%A0%B9%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/"},{"title":"Hello World-0","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-0/"},{"title":"Hello World-3","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-3/"},{"title":"Hello World-4","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-4/"},{"title":"Hello World-2","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-2/"},{"title":"Hello World-8","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-7/"},{"title":"Hello World-8","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-8/"},{"title":"Hello World-9","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-9/"},{"title":"Hello World-6","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-6/"},{"title":"文本向量表示相关的数据集","text":"SNLI基本信息说明 全称：The Stanford Natural Language Inference； 官方链接：https://nlp.stanford.edu/projects/snli/ 下载后的文件名：snli_1.0.zip 文件目录及数据量如下所示： 1234567891011121314(base) ➜ snli_1.0 lltotal 1754136-rw-r--r--@ 1 root root 0B 5 22 2015 Icon?-rw-r--r--@ 1 root root 5.7K 8 29 2015 README.txt-rw-r--r--@ 1 root root 9.3M 8 18 2015 snli_1.0_dev.jsonl-rw-r--r--@ 1 root root 7.2M 8 18 2015 snli_1.0_dev.txt-rw-r--r--@ 1 root root 9.3M 8 18 2015 snli_1.0_test.jsonl-rw-r--r--@ 1 root root 7.2M 8 18 2015 snli_1.0_test.txt-rw-r--r--@ 1 root root 465M 8 18 2015 snli_1.0_train.jsonl-rw-r--r--@ 1 root root 358M 8 18 2015 snli_1.0_train.txt(base) ➜ snli_1.0 wc -l *.jsonl 10000 snli_1.0_dev.jsonl 10000 snli_1.0_test.jsonl 550152 snli_1.0_train.jsonl 任务说明将基础文本与推理假设之间的关系分为三个类别： entailment：蕴含关系、推理关系； contradiction：矛盾关系、对立关系； neutral：中立关系、无关系； 每条文本由不同的人标注5次，取频次最高的标签最为最终标签。举例几条数据如下表所示： Text Judgments Hypothesis A man inspects the uniform of a figure in some East Asian country. contradictionC C C C C The man is sleeping An older and younger man smiling. neutralN N E N N Two men are smiling and laughing at the cats playing on the floor. A black race car starts up in front of a crowd of people. contradictionC C C C C A man is driving down a lonely road. A soccer game with multiple males playing. entailmentE E E E E Some men are playing a sport. A smiling costumed woman is holding an umbrella. neutralN N E C N A happy woman in a fairy costume holds an umbrella. MultiNLI基本信息说明 全称：The Multi-Genre Natural Language Inference； 官方链接：https://cims.nyu.edu/~sbowman/multinli/ 下载后的文件名：multinli_1.0.zip 文件目录及数据量如下所示： 12345678910111213141516(base) ➜ multinli_1.0 lltotal 1880944-rw-r--r--@ 1 root root 0B 2 17 2018 Icon?-rwxr-xr-x@ 1 root root 1.1K 4 16 2018 README.txt-rw-r--r--@ 1 root root 12M 9 4 2017 multinli_1.0_dev_matched.jsonl-rw-r--r--@ 1 root root 10M 9 4 2017 multinli_1.0_dev_matched.txt-rw-r--r--@ 1 root root 13M 9 4 2017 multinli_1.0_dev_mismatched.jsonl-rw-r--r--@ 1 root root 11M 9 4 2017 multinli_1.0_dev_mismatched.txt-rw-r--r--@ 1 root root 470M 9 4 2017 multinli_1.0_train.jsonl-rw-r--r--@ 1 root root 390M 9 4 2017 multinli_1.0_train.txt-rw-r--r--@ 1 root root 192K 4 16 2018 paper.pdf(base) ➜ multinli_1.0 wc -l *.jsonl 10000 multinli_1.0_dev_matched.jsonl 10000 multinli_1.0_dev_mismatched.jsonl 392702 multinli_1.0_train.jsonl 412702 total 任务说明STS基本信息说明 全称：Semantic Textual Similarity； 官方链接：http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark 下载后的文件名：Stsbenchmark.tar.gz 文件目录及数据量如下所示： 12345678910111213(base) ➜ stsbenchmark lltotal 2864-rw-rw-r--@ 1 wangmingchao staff 6.5K 2 24 2017 LICENSE.txt-rw-rw-r--@ 1 wangmingchao staff 2.1K 10 28 2016 correlation.pl-rw-rw-r--@ 1 wangmingchao staff 5.9K 2 24 2017 readme.txt-rw-rw-r--@ 1 wangmingchao staff 250K 2 15 2017 sts-dev.csv-rw-rw-r--@ 1 wangmingchao staff 276K 2 15 2017 sts-test.csv-rw-rw-r--@ 1 wangmingchao staff 880K 2 15 2017 sts-train.csv(base) ➜ stsbenchmark wc -l *.csv 1500 sts-dev.csv 1379 sts-test.csv 5749 sts-train.csv 8628 total 任务说明对两个文本的相似度进行打分，分值为0到5之间的浮点数。该任务可以作为回归任务，也可以简化为类别为5的分类任务。 举例如下： 123455.000 A plane is taking off. An air plane is taking off.3.800 A man is playing a large flute. A man is playing a flute.3.800 A man is spreading shreded cheese on a pizza. A man is spreading shredded cheese on an uncooked pizza.2.600 Three men are playing chess. Two men are playing chess.0.500 A man is smoking. A man is skating. AFS基本信息说明 全称：Argument Facet Similarity； 官方链接：https://nlds.soe.ucsc.edu/node/44 下载后的文件名：Sigdial_16_release_data.zip 文件目录及数据量如下所示： 12345678910111213(base) ➜ stsbenchmark lltotal 2864-rw-rw-r--@ 1 wangmingchao staff 6.5K 2 24 2017 LICENSE.txt-rw-rw-r--@ 1 wangmingchao staff 2.1K 10 28 2016 correlation.pl-rw-rw-r--@ 1 wangmingchao staff 5.9K 2 24 2017 readme.txt-rw-rw-r--@ 1 wangmingchao staff 250K 2 15 2017 sts-dev.csv-rw-rw-r--@ 1 wangmingchao staff 276K 2 15 2017 sts-test.csv-rw-rw-r--@ 1 wangmingchao staff 880K 2 15 2017 sts-train.csv(base) ➜ stsbenchmark wc -l *.csv 1500 sts-dev.csv 1379 sts-test.csv 5749 sts-train.csv 8628 total 任务说明数据集为辩论数据，包含 gun control（枪支管控）、gay marriage（同性恋结婚）、death penalty（死刑） 三个主题。 对两个文本的相似度进行打分，分值为0到5之间的浮点数。该任务可以作为回归任务，也可以简化为类别为5的分类任务。 举例如下： 1234567891011121314151617[ { &quot;regression_label&quot;: 4.333333333, &quot;sentence_2&quot;: &quot;If a person just raped someone (this is just an example) they shouldn`'`t be killed for it, but something like a mass murdering or serial killing should be dealt with by putting them to death.&quot;, &quot;sentence_1&quot;: &quot;Yes there should be a death penalty but it should only be used in extreme circumstances like treason, mass murder, ordering murders from within prison, and killing someone in prison.&quot; }, { &quot;regression_label&quot;: 1.666666667, &quot;sentence_2&quot;: &quot;I am advocating life in prison without the option of parole.&quot;, &quot;sentence_1&quot;: &quot;In every state that retains the death penalty, jurors have the option of sentencing convicted capital murderers to life in prison without the possibility of parole.&quot; }, { &quot;regression_label&quot;: 1.333333333, &quot;sentence_2&quot;: &quot;I would prefer to live in a society where no death penalty was handed out cause there was no crime.&quot;, &quot;sentence_1&quot;: &quot;I am 'for' capital punishment... but if a criminal shows a desire for death over a life sentence, I prefer to be sadistic.&quot; }]","link":"/2022/05/09/NLP%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E7%9B%B8%E5%85%B3%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86/"},{"title":"PyTorch中优化器的源码和显存分析","text":"本文按照论文 An overview of gradient descent optimization algorithms 中提到的优化器顺序和PyTorch中对这些优化器的实现，先了解了解源码，然后再分析一下在训练过程中不同的优化器的显存使用量。 一、SGD优化器torch.Tensor: 是PyTorch中用来做高维的张量运算的基本数据结构。这个类里面会存储数据，也包含了常见的矩阵相加相乘、张量相加相乘等常见运算。 在构造神经网络时，直接使用torch.Tensor这个基础数据结构作为网络中的每个单元是比较不方便的，比如PyTorch中支持网络中的各层自动求导，同时肯定也会有一些数据是中间的临时数据，并不需要求导，甚至于并不属于神经网络的一部分。如果直接使用torch.Tensor，那么哪些Tensor属于神经网络需要求导，哪些Tensor不属于神经网络不需要求导，并不易区分，所以有了torch.nn.Parameter。 torch.nn.Parameter: 该类是构成神经网络的基础单元。它与torch.Tensor的区别是，在模型中（比如torch.nn.Module）所有torch.nn.Parameter类型的属性都会被自动添加到模型的参数中，并可以通过函数Module.parameters()进行迭代。 二、Momentum2.1 公式与源码公式 Momentum SGD的优化公式为： $$ v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta) $$ $$ \\theta = \\theta - v_t$$ 其中 $\\gamma$ 和 $\\eta$ 是超参数，分别为动量和学习率。 源码 PyTorch中关于Momentum SGD的代码简化后如下，PyTorch中SGD源码： 1234567891011121314151617181920212223242526class SGD(Optimizer): def __init__(self, params, defaults): self.state = defaultdict(dict) # 用来缓存优化器状态 def step(self): for group in self.param_groups: momentum = group[&quot;momentum&quot;] # 动量 lr = group[&quot;lr&quot;] # 学习率 for p in group[&quot;params&quot;]: if p.grad is None: # 如果当前参数梯度为None，则不需要更新 continue d_p = p.grad # 梯度 # 由下面这段代码可看出只有第一次执行时需要开辟新缓存，之后都是在更新self.state中的值 state = self.state[p] if &quot;momentum_buffer&quot; not in state: buf = torch.clone(d_p).detach() else: buf = state[&quot;momentum_buffer&quot;] buf.mul_(momentum).add_(d_p) state[&quot;momentum_buffer&quot;] = buf d_p = buf p.add_(d_p, alpha=-lr) # 将（梯度 * -学习率）更新到权重参数上 代码中主要变量含义： self.state：相比于没有动量的SGD，其多了一个字典属性self.state。其key为模型中需要优化的每个参数，value为该参数历史的梯度累积（这里的累积不是指直接相加）。 momentum：超参数，动量，即公式中的 $\\gamma$； lr：超参数，学习率，即公式中的 $\\eta$； p.grad：本次反向传播时计算的梯度值，即公式中的 $\\nabla_{\\theta} J(\\theta)$； 代码中的主要涉及运算的代码行： 第22行代码对应的运算为：$ \\text{buf} = \\text{buf} \\times \\text{momentum} + \\text{d_p} $ 第26行代码对应的运算为：$ \\text{p} = \\text{p} - \\text{lr} \\times \\text{d_p} $ 另外，代码中的学习率的位置与原始公式中学习率的位置是有一点差别的。原始公式中是学习率只作用于本次的新求得的梯度，历史梯度的累积值是不会每次都乘以学习率。代码中是先把本次的新梯度与历史梯度的累积值相加，然后统一乘上学习率。根据代码写出的公式如下： $$ v_t = \\gamma v_{t-1} + \\nabla_{\\theta} J(\\theta) $$ $$ \\theta = \\theta - \\eta v_t$$ 关于代码中的实现对原公式做了修改这一点在PyTorch对文档中也有说明，比如 SGD 中的 NOTE 部分。 2.2 显存占用测试显存占用的代码如下： 1234567891011121314151617181920212223import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)print(&quot;初始化优化器:&quot;, torch.cuda.memory_allocated() - current_memory) # 打印增加的显存for batch_idx in range(3): # 模拟实际训练时有很多个批次 inputs = torch.tensor([1.0] * 2048).cuda() outputs = model(inputs) loss = torch.mean(outputs) loss.backward() current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存 optimizer.step() print(f&quot;第 {batch_idx + 1} 个batch:&quot;, torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出结果如下:# 初始化优化器: 0# 第 1 个batch: 8388608# 第 2 个batch: 0# 第 3 个batch: 0 可以看出只有第一次优化时增加了8M（8388608 = 8M）显存，第二、三次优化时没有增加显存。 模型的参数量为: $ 2048 * 1024 = 2097152 = 2 * 2^{20} = 2M $，从代码中可知属性self.state会给每个参数保存一个历史梯度的累积值，所以Momentum SGD中的参数量与模型的参数量是相同的，也是$2M$。每个浮点数使用4个字节，所以总共是$8M$显存。 只有第1个batch增加了显存，后面的batch都不增加显存的原因是：在第1个之前，属性self.state是空字典，在执行第一个batch的优化时，申请了和模型参数量相同的空间来存储历史梯度的累积值。而在后面的batch中都是对self.state中现有值的修改，不再申请新的显存了。 三、Nesterov accelerated gradient原始公式： $$ v_t = \\gamma v_{t-1} + \\nabla_{\\theta} J(\\theta - \\gamma v_{t-1}) $$ $$ \\theta = \\theta - \\eta v_t$$ PyTorch使用的公式： $$ v_t = \\gamma (\\gamma v_{t-1} + \\nabla_{\\theta} J(\\theta)) + \\nabla_{\\theta} J(\\theta) $$ $$ \\theta = \\theta - \\eta v_t$$ 四、Adagrad4.1 公式与源码公式 优化器Adagrad会给每个参数不同的学习率，其原始公式如下，公式中的 $\\frac{\\eta}{ \\sqrt{G_{t,ii} + \\epsilon } }$ 即为新学习率： $$ g_{t,i} = \\nabla_{\\theta_t} J(\\theta_{t,i}) $$ $$ \\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\eta}{ \\sqrt{G_{t,ii} + \\epsilon } } g_{t,i} $$ 在之前的公式中使用不带右下角标的 $\\theta$ 表示模型的参数，不带右下角标的 $\\theta$ 代表着模型中的所有参数；右下角标中的 $i$ 表示所有参数中的第 $i$ 个参数；右下角标中的 $t$ 表示在对参数的多次迭代过程中第 $t$ 次迭代时的参数； 公式中的 $g_{t,i}$ 表示在第 $t$ 次迭代时，对第 $i$ 个参数求得的梯度； 公式中的 $ G_{t,ii} $ 表示模型中第 $i$ 个参数，从第 $1$ 次迭代到第 $t$ 次迭代过程中所有梯度的和，即如下公式： $$ G_{t,ii} = \\sum^t_{\\text{step}=0} g_{\\text{step},i} $$ 源码 PyTorch中关于优化器Adagrad的代码简化后如下，PyTorch中Adagrad源码 ： 123456789101112131415161718192021222324class Adagrad(Optimizer): def __init__(self, initial_accumulator_value=0): self.state = defaultdict(dict) # 用来缓存优化器状态 for group in self.param_groups: for p in group[&quot;params&quot;]: state = self.state[p] state[&quot;sum&quot;] = torch.full_like(p, initial_accumulator_value) # 默认使用全0初始化 def step(self): for group in self.param_groups: lr = group[&quot;lr&quot;] eps = group[&quot;eps&quot;] for p in group[&quot;params&quot;]: if p.grad is None: continue state = self.state[p] grad, state_sum = p.grad, state[&quot;sum&quot;] state_sum.addcmul_(grad, grad, value=1) std = state_sum.sqrt().add_(eps) p.addcdiv_(grad, std, value=-lr) 代码中主要变量含义： self.state：缓存优化器中所有需要存储的中间状态。 lr：超参数，学习率，即公式中的 $\\eta$； eps：超参数，用于平滑防止除数为0，即公式中的 $\\epsilon$； p.grad：当前参数本次反向传播时计算的梯度值，即公式中的 $g_{t,i}$； state_sum：当前参数历史梯度值之和，即公式中的 $G_{t,ii}$； 代码中的主要涉及运算的代码行： 第23行代码对应的运算为：$ \\text{state_sum} = \\text{state_sum} + \\text{grad} \\times \\text{grad} \\times 1 $ 第24行代码对应的运算为：$ \\text{std} = \\sqrt{\\text{state_sum} + \\text{eps}} $ 第25行代码对应的运算为：$ \\text{p} = \\text{p} - \\text{lr} \\times \\frac{\\text{grad}}{\\text{std}} = \\text{p} - \\frac{\\text{lr}}{\\text{std}} \\times \\text{grad}$ 这里的 $\\frac{\\text{lr}}{\\text{std}}$ 就是公式中的新学习率 $\\frac{\\eta}{ \\sqrt{G_{t,ii} + \\epsilon } }$ 。 4.2 显存占用测试显存占用的代码如下： 1234567891011121314151617181920212223import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存optimizer = torch.optim.Adagrad(model.parameters(), lr=0.1)print(&quot;初始化优化器:&quot;, torch.cuda.memory_allocated() - current_memory) # 打印增加的显存for batch_idx in range(3): inputs = torch.tensor([1.0] * 2048).cuda() outputs = model(inputs) loss = torch.mean(outputs) loss.backward() current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存 optimizer.step() print(f&quot;第 {batch_idx} 个batch:&quot;, torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出如下:# 初始化优化器: 8388608# 第 0 个batch: 0# 第 1 个batch: 0# 第 2 个batch: 0 和带动量的SGD不同，带动量的SGD在初始化时 self.state 为空字典，在第一次迭代时申请了和模型参数量相同大小的显存。而在Adagrad中由上一步的简化后的源码可以看出，是在初始化时就已经申请了和模型参数量相同大小的显存，在迭代过程中是直接在 self.state 中的值上进行修改，不需要再额外申请显存。 表现到测试结果上就是，初始化优化器时显存增加了与模型参数等同的大小的显存，参数量为：$ 2048 * 1024 = 2097152 = 2 * 2^{20} = 2M $，每个浮点数4个字节，共计 $8388608 = 8M$ 显存。在迭代过程中不再申请新的显存。 Reference An overview of gradient descent optimization algorithms: https://arxiv.org/abs/1609.04747 路遥知马力——Momentum: https://zhuanlan.zhihu.com/p/21486826 比Momentum更快：揭开Nesterov Accelerated Gradient的真面目: https://zhuanlan.zhihu.com/p/22810533 PyTorch源码浅析: https://www.52coding.com.cn/2019/05/05/PyTorch0/","link":"/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/PyTorch%E4%B8%AD%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%BA%90%E7%A0%81%E5%92%8C%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"},{"title":"Hello World-5","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } }$$ 这是另一个块状公式：\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\]","link":"/2022/05/09/test/hello-world-5/"},{"title":"PyTorch训练显存分析","text":"最近看到两篇关于PyTorch显存分析的文章，按照文章中的思路和自己对PyTorch使用过程的一些理解做了一些测试和分析，在此记录一下。对于PyTorch在使用时每个步骤使用的显存做两步分析： 通过工具测试出PyTorch在做每个操作时使用的显存； 尽量从公式和代码中计算出该操作使用的显存为什么是这个大小； 一、PyTorch Context 在文章 PyTorch显存机制分析 中将PyTorch Context描述的比较清楚。PyTorch Context就是在GPU中创建一个PyTorch的运行时环境，它会在执行第一个CUDA操作之前创建。而这个东西所占显存的大小与硬件、驱动、CUDA版本、PyTorch版本都有关系。想要从原理分析这个的大小非常困难，下面直接通过测试的方法看一下不同版本的PyTorch其上下文的大小。 1.1 测试环境物理机上的环境信息： 显卡：2080Ti 驱动版本：450.80.02 nvcc版本：V10.2.89 目的要测试不同的PyTorch版本，但是不打算在物理机上安装各个版本的PyTorch，而是使用相应的docker镜像进行测试。 分别对两个不同的机构发布的PyTorch的docker镜像进行测试： PyTorch官方在docker hub上发布的镜像：https://hub.docker.com/r/pytorch/pytorch/tags NVIDIA为自己的GPU专门发布的PyTorch镜像：https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags 所使用的镜像如下表所示： PyTorch版本 PyTorch官方镜像 NVIDIA镜像 PyTorch 1.6 pytorch/pytorch:1.6.0-cuda10.1-cudnn7-runtime nvcr.io/nvidia/pytorch:20.07-py3 PyTorch 1.7 pytorch/pytorch:1.7.0-cuda11.0-cudnn8-runtime nvcr.io/nvidia/pytorch:20.10-py3 PyTorch 1.8 pytorch/pytorch:1.8.0-cuda11.1-cudnn8-runtime nvcr.io/nvidia/pytorch:21.02-py3 PyTorch 1.9 pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime nvcr.io/nvidia/pytorch:21.06-py3 PyTorch 1.10 pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime nvcr.io/nvidia/pytorch:21.10-py3 PyTorch 1.11 nvcr.io/nvidia/pytorch:22.01-py3 1.2 测试方式直接按下述命令执行，然后使用命令 nvidia-smi 查看当前进程所占用的显存大小（此处以镜像包nvcr.io/nvidia/pytorch:20.07-py3为例）： 12345mycomputer@mycomputer:~$ docker run -it --runtime=nvidia nvcr.io/nvidia/pytorch:20.07-py3 /bin/bashroot@5b7fb9dd02fa:/workspace# python3&gt;&gt;&gt; import torch&gt;&gt;&gt; x = torch.tensor([1.0]).cuda()&gt;&gt;&gt; 1.3 测试结果 NVIDIA镜像 PyTorch官方镜像 PyTorch 1.6 591M 783M PyTorch 1.7 625M 825M PyTorch 1.8 693M 941M PyTorch 1.9 761M 1045M PyTorch 1.10 741M 1113M PyTorch 1.11 741M - 可以看出随着版本越高，PyTorch Context占用的显存越大，这个符合一般的理解。随着版本升高，一般都是往里加新功能，很少会减去功能的。 这里不太确定的是NVIDIA发布的镜像的PyTorch Context明显比PyTorch官方发布的镜像要小。现在还不确定是真实的就是小一些，还是上述测试方法有问题。 1.4 作用在有些场景下不同镜像包之间的这几百M的差异并不重要，但是在有些场景下就有些重要了。比如仅做推理的线上服务，所有的算法服务都要同时部署，但是这些服务并不是每时每刻都在进行推理，所以此时GPU的计算资源是有空闲的，而GPU的显存却是紧缺的。如果能够降低PyTorch运行时上下文占用的显存就可以多部署一些服务。 二、训练过程显存分析在该部分显存分析时使用函数 torch.cuda.memory_allocated() 获取当前所有张量的显存大小，该函数的返回值的单位是 字节。注意该函数获取到的显存大小与 nvidia-smi 获取到的显存大小是不同的，它们之间的差别在最后一部分说明。在该部分的分析过程中都使用函数 torch.cuda.memory_allocated() 获取的结果。 在这里为了简化，使用单层全连接作为模型。 2.1 模型参数占显存大小123456import torchcurrent_memory = torch.cuda.memory_allocated() # 记录当前使用的显存model = torch.nn.Linear(2048, 1024, bias=False).cuda()print(torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出为: 8388608 此时的参数量为 2M（2048*1024），显存大小为 8M（8388608），符合常规认识的每个参数用4个字节存储。 2.2 输入占显存大小1234567import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存inputs = torch.tensor([1.0] * 2048).cuda()print(torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出为: 8192 此时参数量为 2k（2048），显存大小为 8k（8192），也没有问题。 2.3 前向传播占显存大小12345678import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()inputs = torch.tensor([1.0] * 2048).cuda()current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存outputs = model(inputs)print(torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出为: 4096 outpus的参数量为 1k（1024），显存大小为 4k（4096）。 2.4 反向传播计算梯度占显存大小2.4.1 反向传播简述链式求导公式为： $$ \\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial w} $$ 这里的 $\\frac{\\partial E}{\\partial y}$ 是传入导Linear层的梯度，$\\frac{\\partial y}{\\partial w}$ 是求解Linear层的梯度。 忽略 bias 后Linear层前向传播时的公式为 $y = w*x $，求导后为：$\\frac{\\partial y}{\\partial w} = x$； 参照下面简化的Linear层的代码（该代码删掉了大量判断条件、输入参数、返回值、bias等，仅作为示意）来看，前向传播forward()函数为输入乘以权重矩阵，反向传播backward()为输入的梯度乘以本层求导后的结果$x$（即input）； 函数backward()计算出的梯度grad_weight维度与该Linear层的权重参数相同，该值是需要消耗显存进行存储的。 123456789101112class Linear(Function): def forward(self, input, weight): self.save_for_backward(input, weight) # 将输入和权重存储给反向传播时使用 output = input.mm(weight.t()) return output def backward(self, grad_output): input, weight = self.saved_tensors # 就是第4行代码中存储的输入和权重 grad_weight = grad_output.t().mm(input) return grad_weight 关于PyTorch中反向传播更详细的内容可以参考官方文档 自动求导机制 和 扩展PyTorch。 2.4.2 反向传播显存存储计算出的loss值本身也需要少量的显存空间，先看一下： 123456789import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()inputs = torch.tensor([1.0] * 2048).cuda()outputs = model(inputs)current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存loss = torch.mean(outputs)print(torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出为: 512 然后看反向传播计算梯度： 12345678910import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()inputs = torch.tensor([1.0] * 2048).cuda()outputs = model(inputs)loss = torch.mean(outputs)current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存loss.backward()print(torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出为: 8388608 梯度的参数量等同于权重$w$，为 2M（2048 * 1024），消耗的显存为 8M（8388608）； 2.5 参数更新时的优化器状态占显存大小2.5.1 SGD优化简述不带动量的SGD优化公式为： $$ \\theta = \\theta - \\eta \\nabla_{\\theta} J(\\theta) $$ 带动量的SGD的优化公式为： $$ v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta) $$ $$ \\theta = \\theta - v_t$$ 简化后的SGD代码如下（源码链接），该代码删掉了大量判断条件、输入参数、返回值等，仅作为示意。代码中的lr为学习率，对应公式中的$\\eta$；代码中的momentum为动量，对应公式中的$\\gamma$。 123456789101112131415161718192021222324252627class SGD(Optimizer): def __init__(self, params, defaults): self.state = defaultdict(dict) # 用来缓存优化器状态 def step(self, closure=None): for group in self.param_groups: momentum = group[&quot;momentum&quot;] # 动量 lr = group[&quot;lr&quot;] # 学习率 for p in group[&quot;params&quot;]: if p.grad is None: # 如果当前参数梯度为None，则不需要更新 continue d_p = p.grad # 梯度 if momentum != 0: # 如果动量不为0，则根据动量重新计算梯度 # 由下面这段代码可看出只有第一次执行时需要开辟新缓存，之后都是在更新self.state中的值 state = self.state[p] if &quot;momentum_buffer&quot; not in state: buf = torch.clone(d_p).detach() else: buf = state[&quot;momentum_buffer&quot;] buf.mul_(momentum).add_(d_p) state[&quot;momentum_buffer&quot;] = buf d_p = d_p.add(buf, alpha=momentum) p.add_(d_p, alpha=-lr) # 将（梯度 * -学习率）更新到权重参数上 2.5.2 SGD优化时的显存使用SGD作为优化器，不使用动量 1234567891011121314import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()optimizer = torch.optim.SGD(model.parameters(), lr=0.1)inputs = torch.tensor([1.0] * 2048).cuda()outputs = model(inputs)loss = torch.mean(outputs)loss.backward()current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存optimizer.step()print(torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出为: 0 由上面的SGD代码可知，当不使用动量时，优化器所做的工作其实就是一行代码 p.add_(d_p, alpha=-lr)，这里面只使用到了参数权重和其梯度，优化器本身没有任何需要存储的变量，所以新增的显存为0。 使用SGD作为优化器，使用动量 1234567891011121314import torchmodel = torch.nn.Linear(2048, 1024, bias=False).cuda()optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)inputs = torch.tensor([1.0] * 2048).cuda()outputs = model(inputs)loss = torch.mean(outputs)loss.backward()current_memory = torch.cuda.memory_allocated() # 记录当前使用的显存optimizer.step()print(torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出为: 8388608 由上面的SGD代码可知，当使用动量时，优化器需要在self.state中为每个参数存储一个momentum_buffer，每有一个参数，就需要一个对应的momentum_buffer，所以新增的量与参数量是相同的，新增的显存为 8M（8388608）。 这里只是以最简单的SGD优化器为例，优化器还有更多类型，不同的优化器根据其存储的中间量的不同，需要的显存也不尽相同，需要具体分析。 2.6 训练过程总体分析上面把训练过程中的每个步骤都详细分析过了，下面总体看一下： 12345678910111213import torchcurrent_memory = torch.cuda.memory_allocated() # 记录当前使用的显存model = torch.nn.Linear(2048, 1024, bias=False).cuda()optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) # SGD使用动量inputs = torch.tensor([1.0] * 2048).cuda()outputs = model(inputs)loss = torch.mean(outputs)loss.backward()optimizer.step()print(torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出：25178624 这里的输出值25178624字节的显存由以下部分构成： 8388608字节: 8M，模型参数model消耗的显存； 8192字节: 8k，输入inputs消耗的显存； 4096字节: 4k，输出outputs消耗的显存； 512字节: 512字节，存储loss消耗的显存； 8388608字节: 8M，模型model中每个参数的梯度消耗的显存； 8388608字节: 8M，优化器optimizer消耗的显存； 另外，模拟一下实际训练时会训练很多个batch，代码如下，其输出结果和之前是相同的。 1234567891011121314import torchcurrent_memory = torch.cuda.memory_allocated() # 记录当前使用的显存model = torch.nn.Linear(2048, 1024, bias=False).cuda()optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) # SGD使用动量for _ in range(10): inputs = torch.tensor([1.0] * 2048).cuda() outputs = model(inputs) loss = torch.mean(outputs) loss.backward() optimizer.step()print(torch.cuda.memory_allocated() - current_memory) # 打印增加的显存# 输出：25178624 2.7 总结这里以一个最简单的Linear作为模型，分析了在整个训练过程中，各个环节所使用的显存大小。后面还可以对各个常见的模型结构CNN、LSTM、Transformer等进行分析，其整个分析思路和上面是相同的，所以只要完全清楚这些模型的细节，显存分析只是一个体力活。 Reference 深度学习中GPU和显存分析: https://zhuanlan.zhihu.com/p/31558973 PyTorch显存机制分析: https://zhuanlan.zhihu.com/p/424512257 PyTorch: https://github.com/pytorch/pytorch","link":"/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/Pytorch%E8%AE%AD%E7%BB%83%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"显存","slug":"显存","link":"/tags/%E6%98%BE%E5%AD%98/"},{"name":"优化器","slug":"优化器","link":"/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"},{"name":"测试","slug":"测试","link":"/tags/%E6%B5%8B%E8%AF%95/"},{"name":"MacOS","slug":"MacOS","link":"/tags/MacOS/"},{"name":"向量表示","slug":"向量表示","link":"/tags/%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"}],"categories":[{"name":"NLP工程","slug":"NLP工程","link":"/categories/NLP%E5%B7%A5%E7%A8%8B/"},{"name":"测试类文章","slug":"测试类文章","link":"/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/"},{"name":"其他","slug":"其他","link":"/categories/%E5%85%B6%E4%BB%96/"},{"name":"父类-测试文章","slug":"父类-测试文章","link":"/categories/%E7%88%B6%E7%B1%BB-%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"},{"name":"PyTorch","slug":"NLP工程/PyTorch","link":"/categories/NLP%E5%B7%A5%E7%A8%8B/PyTorch/"},{"name":"子类-测试文章","slug":"父类-测试文章/子类-测试文章","link":"/categories/%E7%88%B6%E7%B1%BB-%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/%E5%AD%90%E7%B1%BB-%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"},{"name":"NLP算法","slug":"NLP算法","link":"/categories/NLP%E7%AE%97%E6%B3%95/"},{"name":"文本向量表示","slug":"NLP算法/文本向量表示","link":"/categories/NLP%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"}]}