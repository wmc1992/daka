<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>PyTorch中优化器的源码和显存分析 - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="本文按照论文 An overview of gradient descent optimization algorithms 中提到的优化器顺序和PyTorch中对这些优化器的实现，先了解了解源码，然后再分析一下在训练过程中不同的优化器的显存使用量。 一、SGD优化器torch.Tensor: 是PyTorch中用来做高维的张量运算的基本数据结构。这个类里面会存储数据，也包含了常见的矩阵相加相乘、"><meta property="og:type" content="blog"><meta property="og:title" content="PyTorch中优化器的源码和显存分析"><meta property="og:url" content="http://example.com/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/PyTorch%E4%B8%AD%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%BA%90%E7%A0%81%E5%92%8C%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="本文按照论文 An overview of gradient descent optimization algorithms 中提到的优化器顺序和PyTorch中对这些优化器的实现，先了解了解源码，然后再分析一下在训练过程中不同的优化器的显存使用量。 一、SGD优化器torch.Tensor: 是PyTorch中用来做高维的张量运算的基本数据结构。这个类里面会存储数据，也包含了常见的矩阵相加相乘、"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:published_time" content="2022-05-08T16:00:17.885Z"><meta property="article:modified_time" content="2022-05-08T16:00:17.886Z"><meta property="article:author" content="John Doe"><meta property="article:tag" content="PyTorch"><meta property="article:tag" content="显存"><meta property="article:tag" content="优化器"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/PyTorch%E4%B8%AD%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%BA%90%E7%A0%81%E5%92%8C%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"},"headline":"PyTorch中优化器的源码和显存分析","image":["http://example.com/img/og_image.png"],"datePublished":"2022-05-08T16:00:17.885Z","dateModified":"2022-05-08T16:00:17.886Z","author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"本文按照论文 An overview of gradient descent optimization algorithms 中提到的优化器顺序和PyTorch中对这些优化器的实现，先了解了解源码，然后再分析一下在训练过程中不同的优化器的显存使用量。 一、SGD优化器torch.Tensor: 是PyTorch中用来做高维的张量运算的基本数据结构。这个类里面会存储数据，也包含了常见的矩阵相加相乘、"}</script><link rel="canonical" href="http://example.com/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/PyTorch%E4%B8%AD%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%BA%90%E7%A0%81%E5%92%8C%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/wmc1992/daka"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-05-08T16:00:17.885Z" title="2022/5/9 上午12:00:17">2022-05-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-05-08T16:00:17.886Z" title="2022/5/9 上午12:00:17">2022-05-09</time></span><span class="level-item"><a class="link-muted" href="/categories/NLP%E5%B7%A5%E7%A8%8B/">NLP工程</a><span> / </span><a class="link-muted" href="/categories/NLP%E5%B7%A5%E7%A8%8B/PyTorch/">PyTorch</a></span><span class="level-item">17 minutes read (About 2519 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">PyTorch中优化器的源码和显存分析</h1><div class="content"><p>本文按照论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04747">An overview of gradient descent optimization algorithms</a> 中提到的优化器顺序和PyTorch中对这些优化器的实现，先了解了解源码，然后再分析一下在训练过程中不同的优化器的显存使用量。</p>
<h2 id="一、SGD优化器"><a href="#一、SGD优化器" class="headerlink" title="一、SGD优化器"></a>一、SGD优化器</h2><p><code>torch.Tensor</code>: 是PyTorch中用来做高维的张量运算的基本数据结构。这个类里面会存储数据，也包含了常见的矩阵相加相乘、张量相加相乘等常见运算。</p>
<span id="more"></span>

<p>在构造神经网络时，直接使用<code>torch.Tensor</code>这个基础数据结构作为网络中的每个单元是比较不方便的，比如PyTorch中支持网络中的各层自动求导，同时肯定也会有一些数据是中间的临时数据，并不需要求导，甚至于并不属于神经网络的一部分。如果直接使用<code>torch.Tensor</code>，那么哪些Tensor属于神经网络需要求导，哪些Tensor不属于神经网络不需要求导，并不易区分，所以有了<code>torch.nn.Parameter</code>。</p>
<p><code>torch.nn.Parameter</code>: 该类是构成神经网络的基础单元。它与<code>torch.Tensor</code>的区别是，在模型中（比如<code>torch.nn.Module</code>）所有<code>torch.nn.Parameter</code>类型的属性都会被自动添加到模型的参数中，并可以通过函数<code>Module.parameters()</code>进行迭代。</p>
<h2 id="二、Momentum"><a href="#二、Momentum" class="headerlink" title="二、Momentum"></a>二、Momentum</h2><h3 id="2-1-公式与源码"><a href="#2-1-公式与源码" class="headerlink" title="2.1 公式与源码"></a>2.1 公式与源码</h3><p><strong>公式</strong></p>
<p>Momentum SGD的优化公式为：</p>
<p>$$ v_t &#x3D; \gamma v_{t-1} + \eta \nabla_{\theta} J(\theta) $$</p>
<p>$$ \theta &#x3D; \theta - v_t$$</p>
<p>其中 $\gamma$ 和 $\eta$ 是超参数，分别为动量和学习率。</p>
<p><strong>源码</strong></p>
<p>PyTorch中关于Momentum SGD的代码简化后如下，<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/v1.11.0-rc5/torch/optim/sgd.py">PyTorch中SGD源码</a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SGD</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params, defaults</span>):</span><br><span class="line">        self.state = defaultdict(<span class="built_in">dict</span>)  <span class="comment"># 用来缓存优化器状态</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            momentum = group[<span class="string">&quot;momentum&quot;</span>]  <span class="comment"># 动量</span></span><br><span class="line">            lr = group[<span class="string">&quot;lr&quot;</span>]  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># 如果当前参数梯度为None，则不需要更新</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                d_p = p.grad  <span class="comment"># 梯度</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 由下面这段代码可看出只有第一次执行时需要开辟新缓存，之后都是在更新self.state中的值</span></span><br><span class="line">                state = self.state[p]</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;momentum_buffer&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">                    buf = torch.clone(d_p).detach()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    buf = state[<span class="string">&quot;momentum_buffer&quot;</span>]</span><br><span class="line">                    buf.mul_(momentum).add_(d_p)</span><br><span class="line">                state[<span class="string">&quot;momentum_buffer&quot;</span>] = buf</span><br><span class="line">                d_p = buf</span><br><span class="line"></span><br><span class="line">                p.add_(d_p, alpha=-lr)  <span class="comment"># 将（梯度 * -学习率）更新到权重参数上</span></span><br></pre></td></tr></table></figure>

<p>代码中主要变量含义：</p>
<ul>
<li><code>self.state</code>：相比于没有动量的SGD，其多了一个字典属性<code>self.state</code>。其key为模型中需要优化的每个参数，value为该参数历史的梯度累积（这里的累积不是指直接相加）。</li>
<li><code>momentum</code>：超参数，动量，即公式中的 $\gamma$；</li>
<li><code>lr</code>：超参数，学习率，即公式中的 $\eta$；</li>
<li><code>p.grad</code>：本次反向传播时计算的梯度值，即公式中的 $\nabla_{\theta} J(\theta)$；</li>
</ul>
<p>代码中的主要涉及运算的代码行：</p>
<ul>
<li>第22行代码对应的运算为：$ \text{buf} &#x3D; \text{buf} \times \text{momentum} + \text{d_p} $</li>
<li>第26行代码对应的运算为：$ \text{p} &#x3D; \text{p} - \text{lr} \times \text{d_p} $</li>
</ul>
<p>另外，代码中的学习率的位置与原始公式中学习率的位置是有一点差别的。原始公式中是学习率只作用于本次的新求得的梯度，历史梯度的累积值是不会每次都乘以学习率。代码中是先把本次的新梯度与历史梯度的累积值相加，然后统一乘上学习率。根据代码写出的公式如下：</p>
<p>$$ v_t &#x3D; \gamma v_{t-1} + \nabla_{\theta} J(\theta) $$</p>
<p>$$ \theta &#x3D; \theta - \eta v_t$$</p>
<p>关于代码中的实现对原公式做了修改这一点在PyTorch对文档中也有说明，比如 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGD</a> 中的 <code>NOTE</code> 部分。</p>
<h3 id="2-2-显存占用"><a href="#2-2-显存占用" class="headerlink" title="2.2 显存占用"></a>2.2 显存占用</h3><p>测试显存占用的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line"></span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;初始化优化器:&quot;</span>, torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):  <span class="comment"># 模拟实际训练时有很多个批次</span></span><br><span class="line">    inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = torch.mean(outputs)</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;第 <span class="subst">&#123;batch_idx + <span class="number">1</span>&#125;</span> 个batch:&quot;</span>, torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果如下:</span></span><br><span class="line"><span class="comment">#    初始化优化器: 0</span></span><br><span class="line"><span class="comment">#    第 1 个batch: 8388608</span></span><br><span class="line"><span class="comment">#    第 2 个batch: 0</span></span><br><span class="line"><span class="comment">#    第 3 个batch: 0</span></span><br></pre></td></tr></table></figure>

<p>可以看出只有第一次优化时增加了8M（8388608 &#x3D; 8M）显存，第二、三次优化时没有增加显存。</p>
<p>模型的参数量为: $ 2048 * 1024 &#x3D; 2097152 &#x3D; 2 * 2^{20} &#x3D; 2M $，从代码中可知属性<code>self.state</code>会给每个参数保存一个历史梯度的累积值，所以Momentum SGD中的参数量与模型的参数量是相同的，也是$2M$。每个浮点数使用4个字节，所以总共是$8M$显存。</p>
<p>只有第1个batch增加了显存，后面的batch都不增加显存的原因是：在第1个之前，属性<code>self.state</code>是空字典，在执行第一个batch的优化时，申请了和模型参数量相同的空间来存储历史梯度的累积值。而在后面的batch中都是对<code>self.state</code>中现有值的修改，不再申请新的显存了。</p>
<h2 id="三、Nesterov-accelerated-gradient"><a href="#三、Nesterov-accelerated-gradient" class="headerlink" title="三、Nesterov accelerated gradient"></a>三、Nesterov accelerated gradient</h2><p>原始公式：</p>
<p>$$ v_t &#x3D; \gamma v_{t-1} + \nabla_{\theta} J(\theta - \gamma v_{t-1}) $$</p>
<p>$$ \theta &#x3D; \theta - \eta v_t$$</p>
<p>PyTorch使用的公式：</p>
<p>$$ v_t &#x3D; \gamma (\gamma v_{t-1} + \nabla_{\theta} J(\theta)) + \nabla_{\theta} J(\theta) $$</p>
<p>$$ \theta &#x3D; \theta - \eta  v_t$$</p>
<h2 id="四、Adagrad"><a href="#四、Adagrad" class="headerlink" title="四、Adagrad"></a>四、Adagrad</h2><h3 id="4-1-公式与源码"><a href="#4-1-公式与源码" class="headerlink" title="4.1 公式与源码"></a>4.1 公式与源码</h3><p><strong>公式</strong></p>
<p>优化器Adagrad会给每个参数不同的学习率，其原始公式如下，公式中的 $\frac{\eta}{ \sqrt{G_{t,ii} + \epsilon } }$ 即为新学习率：</p>
<p>$$ g_{t,i} &#x3D; \nabla_{\theta_t} J(\theta_{t,i}) $$</p>
<p>$$ \theta_{t+1,i} &#x3D; \theta_{t,i} - \frac{\eta}{ \sqrt{G_{t,ii} + \epsilon } } g_{t,i} $$</p>
<p>在之前的公式中使用不带右下角标的 $\theta$ 表示模型的参数，不带右下角标的 $\theta$ 代表着模型中的所有参数；右下角标中的 $i$ 表示所有参数中的第 $i$ 个参数；右下角标中的 $t$ 表示在对参数的多次迭代过程中第 $t$ 次迭代时的参数；</p>
<p>公式中的 $g_{t,i}$ 表示在第 $t$ 次迭代时，对第 $i$ 个参数求得的梯度；</p>
<p>公式中的 $ G_{t,ii} $ 表示模型中第 $i$ 个参数，从第 $1$ 次迭代到第 $t$ 次迭代过程中所有梯度的和，即如下公式：</p>
<p>$$ G_{t,ii} &#x3D; \sum^t_{\text{step}&#x3D;0} g_{\text{step},i} $$</p>
<p><strong>源码</strong></p>
<p>PyTorch中关于优化器Adagrad的代码简化后如下，<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/v1.11.0-rc5/torch/optim/adagrad.py">PyTorch中Adagrad源码</a> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Adagrad</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, initial_accumulator_value=<span class="number">0</span></span>):</span><br><span class="line">        self.state = defaultdict(<span class="built_in">dict</span>)  <span class="comment"># 用来缓存优化器状态</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">&quot;sum&quot;</span>] = torch.full_like(p, initial_accumulator_value)  <span class="comment"># 默认使用全0初始化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            lr = group[<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">            eps = group[<span class="string">&quot;eps&quot;</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                state = self.state[p]</span><br><span class="line">                grad, state_sum = p.grad, state[<span class="string">&quot;sum&quot;</span>]</span><br><span class="line"></span><br><span class="line">                state_sum.addcmul_(grad, grad, value=<span class="number">1</span>)</span><br><span class="line">                std = state_sum.sqrt().add_(eps)</span><br><span class="line">                p.addcdiv_(grad, std, value=-lr)</span><br></pre></td></tr></table></figure>

<p>代码中主要变量含义：</p>
<ul>
<li><code>self.state</code>：缓存优化器中所有需要存储的中间状态。</li>
<li><code>lr</code>：超参数，学习率，即公式中的 $\eta$；</li>
<li><code>eps</code>：超参数，用于平滑防止除数为0，即公式中的 $\epsilon$；</li>
<li><code>p.grad</code>：当前参数本次反向传播时计算的梯度值，即公式中的 $g_{t,i}$；</li>
<li><code>state_sum</code>：当前参数历史梯度值之和，即公式中的 $G_{t,ii}$；</li>
</ul>
<p>代码中的主要涉及运算的代码行：</p>
<ul>
<li>第23行代码对应的运算为：$ \text{state_sum} &#x3D; \text{state_sum} + \text{grad} \times \text{grad} \times 1 $</li>
<li>第24行代码对应的运算为：$ \text{std} &#x3D; \sqrt{\text{state_sum} + \text{eps}} $</li>
<li>第25行代码对应的运算为：$ \text{p} &#x3D; \text{p} - \text{lr} \times \frac{\text{grad}}{\text{std}} &#x3D; \text{p} - \frac{\text{lr}}{\text{std}} \times \text{grad}$</li>
</ul>
<p>这里的 $\frac{\text{lr}}{\text{std}}$ 就是公式中的新学习率 $\frac{\eta}{ \sqrt{G_{t,ii} + \epsilon } }$ 。</p>
<h3 id="4-2-显存占用"><a href="#4-2-显存占用" class="headerlink" title="4.2 显存占用"></a>4.2 显存占用</h3><p>测试显存占用的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line"></span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">optimizer = torch.optim.Adagrad(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;初始化优化器:&quot;</span>, torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = torch.mean(outputs)</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;第 <span class="subst">&#123;batch_idx&#125;</span> 个batch:&quot;</span>, torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出如下:</span></span><br><span class="line"><span class="comment">#    初始化优化器: 8388608</span></span><br><span class="line"><span class="comment">#    第 0 个batch: 0</span></span><br><span class="line"><span class="comment">#    第 1 个batch: 0</span></span><br><span class="line"><span class="comment">#    第 2 个batch: 0</span></span><br></pre></td></tr></table></figure>

<p>和带动量的SGD不同，带动量的SGD在初始化时 <code>self.state</code> 为空字典，在第一次迭代时申请了和模型参数量相同大小的显存。而在Adagrad中由上一步的简化后的源码可以看出，是在初始化时就已经申请了和模型参数量相同大小的显存，在迭代过程中是直接在 <code>self.state</code> 中的值上进行修改，不需要再额外申请显存。</p>
<p>表现到测试结果上就是，初始化优化器时显存增加了与模型参数等同的大小的显存，参数量为：$ 2048 * 1024 &#x3D; 2097152 &#x3D; 2 * 2^{20} &#x3D; 2M $，每个浮点数4个字节，共计 $8388608 &#x3D; 8M$ 显存。在迭代过程中不再申请新的显存。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><p>An overview of gradient descent optimization algorithms: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04747">https://arxiv.org/abs/1609.04747</a></p>
</li>
<li><p>路遥知马力——Momentum: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21486826">https://zhuanlan.zhihu.com/p/21486826</a></p>
</li>
<li><p>比Momentum更快：揭开Nesterov Accelerated Gradient的真面目: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/22810533">https://zhuanlan.zhihu.com/p/22810533</a></p>
</li>
<li><p>PyTorch源码浅析: <a target="_blank" rel="noopener" href="https://www.52coding.com.cn/2019/05/05/PyTorch0/">https://www.52coding.com.cn/2019/05/05/PyTorch0/</a></p>
</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>PyTorch中优化器的源码和显存分析</p><p><a href="http://example.com/2022/05/09/NLP工程/PyTorch/性能优化/PyTorch中优化器的源码和显存分析/">http://example.com/2022/05/09/NLP工程/PyTorch/性能优化/PyTorch中优化器的源码和显存分析/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>John Doe</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-05-09</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-05-09</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/PyTorch/">PyTorch</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%98%BE%E5%AD%98/">显存</a><a class="link-muted mr-2" rel="tag" href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/">优化器</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/Pytorch%E8%AE%AD%E7%BB%83%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">PyTorch训练显存分析</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/05/09/%E5%85%B6%E4%BB%96/MacOS%E5%9C%A8%E6%A0%B9%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/"><span class="level-item">MacOS在根目录下新建文件夹</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#一、SGD优化器"><span class="level-left"><span class="level-item">1</span><span class="level-item">一、SGD优化器</span></span></a></li><li><a class="level is-mobile" href="#二、Momentum"><span class="level-left"><span class="level-item">2</span><span class="level-item">二、Momentum</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-公式与源码"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">2.1 公式与源码</span></span></a></li><li><a class="level is-mobile" href="#2-2-显存占用"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">2.2 显存占用</span></span></a></li></ul></li><li><a class="level is-mobile" href="#三、Nesterov-accelerated-gradient"><span class="level-left"><span class="level-item">3</span><span class="level-item">三、Nesterov accelerated gradient</span></span></a></li><li><a class="level is-mobile" href="#四、Adagrad"><span class="level-left"><span class="level-item">4</span><span class="level-item">四、Adagrad</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#4-1-公式与源码"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">4.1 公式与源码</span></span></a></li><li><a class="level is-mobile" href="#4-2-显存占用"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">4.2 显存占用</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">5</span><span class="level-item">Reference</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/NLP%E5%B7%A5%E7%A8%8B/"><span class="level-start"><span class="level-item">NLP工程</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/NLP%E5%B7%A5%E7%A8%8B/PyTorch/"><span class="level-start"><span class="level-item">PyTorch</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/NLP%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">NLP算法</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/NLP%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"><span class="level-start"><span class="level-item">文本向量表示</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E5%85%B6%E4%BB%96/"><span class="level-start"><span class="level-item">其他</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/"><span class="level-start"><span class="level-item">测试类文章</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%88%B6%E7%B1%BB-%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"><span class="level-start"><span class="level-item">父类-测试文章</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E7%88%B6%E7%B1%BB-%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/%E5%AD%90%E7%B1%BB-%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"><span class="level-start"><span class="level-item">子类-测试文章</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/MacOS/"><span class="tag">MacOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PyTorch/"><span class="tag">PyTorch</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"><span class="tag">优化器</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"><span class="tag">向量表示</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%98%BE%E5%AD%98/"><span class="tag">显存</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B5%8B%E8%AF%95/"><span class="tag">测试</span><span class="tag">10</span></a></div></div></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.906Z">2022-05-09</time></p><p class="title"><a href="/2022/05/09/test/hello-world-8/">Hello World-8</a></p><p class="categories"><a href="/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.906Z">2022-05-09</time></p><p class="title"><a href="/2022/05/09/test/hello-world-9/">Hello World-9</a></p><p class="categories"><a href="/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.905Z">2022-05-09</time></p><p class="title"><a href="/2022/05/09/test/hello-world-7/">Hello World-8</a></p><p class="categories"><a href="/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.905Z">2022-05-09</time></p><p class="title"><a href="/2022/05/09/test/hello-world-6/">Hello World-6</a></p><p class="categories"><a href="/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.904Z">2022-05-09</time></p><p class="title"><a href="/2022/05/09/test/hello-world-3/">Hello World-3</a></p><p class="categories"><a href="/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2022 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/wmc1992/daka"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>