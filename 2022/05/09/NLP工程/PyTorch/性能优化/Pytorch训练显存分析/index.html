<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>PyTorch训练显存分析 - wmc1992</title><link rel="manifest" href="/daka/manifest.json"><meta name="application-name" content="wmc1992"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="wmc1992"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="最近看到两篇关于PyTorch显存分析的文章，按照文章中的思路和自己对PyTorch使用过程的一些理解做了一些测试和分析，在此记录一下。对于PyTorch在使用时每个步骤使用的显存做两步分析：  通过工具测试出PyTorch在做每个操作时使用的显存； 尽量从公式和代码中计算出该操作使用的显存为什么是这个大小；  一、PyTorch Context"><meta property="og:type" content="blog"><meta property="og:title" content="PyTorch训练显存分析"><meta property="og:url" content="http://mingchao.wang/daka/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/Pytorch%E8%AE%AD%E7%BB%83%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"><meta property="og:site_name" content="wmc1992"><meta property="og:description" content="最近看到两篇关于PyTorch显存分析的文章，按照文章中的思路和自己对PyTorch使用过程的一些理解做了一些测试和分析，在此记录一下。对于PyTorch在使用时每个步骤使用的显存做两步分析：  通过工具测试出PyTorch在做每个操作时使用的显存； 尽量从公式和代码中计算出该操作使用的显存为什么是这个大小；  一、PyTorch Context"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://mingchao.wang/daka/img/og_image.png"><meta property="article:published_time" content="2022-05-08T16:00:17.886Z"><meta property="article:modified_time" content="2022-05-08T16:00:17.886Z"><meta property="article:author" content="mingchao.wang"><meta property="article:tag" content="PyTorch"><meta property="article:tag" content="显存"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/daka/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://mingchao.wang/daka/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/Pytorch%E8%AE%AD%E7%BB%83%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"},"headline":"PyTorch训练显存分析","image":["http://mingchao.wang/daka/img/og_image.png"],"datePublished":"2022-05-08T16:00:17.886Z","dateModified":"2022-05-08T16:00:17.886Z","author":{"@type":"Person","name":"mingchao.wang"},"publisher":{"@type":"Organization","name":"wmc1992","logo":{"@type":"ImageObject","url":"http://mingchao.wang/img/logo.svg"}},"description":"最近看到两篇关于PyTorch显存分析的文章，按照文章中的思路和自己对PyTorch使用过程的一些理解做了一些测试和分析，在此记录一下。对于PyTorch在使用时每个步骤使用的显存做两步分析：  通过工具测试出PyTorch在做每个操作时使用的显存； 尽量从公式和代码中计算出该操作使用的显存为什么是这个大小；  一、PyTorch Context"}</script><link rel="canonical" href="http://mingchao.wang/daka/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/Pytorch%E8%AE%AD%E7%BB%83%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"><link rel="icon" href="/daka/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/daka/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/daka/"><img src="/daka/img/logo.svg" alt="wmc1992" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/daka/">Home</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/wmc1992/daka"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-05-08T16:00:17.886Z" title="2022/5/9 上午12:00:17">2022-05-09</time>发表</span><span class="level-item"><time dateTime="2022-05-08T16:00:17.886Z" title="2022/5/9 上午12:00:17">2022-05-09</time>更新</span><span class="level-item"><a class="link-muted" href="/daka/categories/NLP%E5%B7%A5%E7%A8%8B/">NLP工程</a><span> / </span><a class="link-muted" href="/daka/categories/NLP%E5%B7%A5%E7%A8%8B/PyTorch/">PyTorch</a></span><span class="level-item">20 分钟读完 (大约3045个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">PyTorch训练显存分析</h1><div class="content"><p>最近看到两篇关于PyTorch显存分析的文章，按照文章中的思路和自己对PyTorch使用过程的一些理解做了一些测试和分析，在此记录一下。对于PyTorch在使用时每个步骤使用的显存做两步分析：</p>
<ol>
<li>通过工具测试出PyTorch在做每个操作时使用的显存；</li>
<li>尽量从公式和代码中计算出该操作使用的显存为什么是这个大小；</li>
</ol>
<h2 id="一、PyTorch-Context"><a href="#一、PyTorch-Context" class="headerlink" title="一、PyTorch Context"></a>一、PyTorch Context</h2><span id="more"></span>

<p>在文章 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/424512257">PyTorch显存机制分析</a> 中将PyTorch Context描述的比较清楚。PyTorch Context就是在GPU中创建一个PyTorch的运行时环境，它会在执行第一个CUDA操作之前创建。而这个东西所占显存的大小与硬件、驱动、CUDA版本、PyTorch版本都有关系。想要从原理分析这个的大小非常困难，下面直接通过测试的方法看一下不同版本的PyTorch其上下文的大小。</p>
<h3 id="1-1-测试环境"><a href="#1-1-测试环境" class="headerlink" title="1.1 测试环境"></a>1.1 测试环境</h3><p>物理机上的环境信息：</p>
<ul>
<li><p>显卡：2080Ti</p>
</li>
<li><p>驱动版本：450.80.02</p>
</li>
<li><p>nvcc版本：V10.2.89</p>
</li>
</ul>
<p>目的要测试不同的PyTorch版本，但是不打算在物理机上安装各个版本的PyTorch，而是使用相应的docker镜像进行测试。</p>
<p>分别对两个不同的机构发布的PyTorch的docker镜像进行测试：</p>
<ul>
<li><p>PyTorch官方在docker hub上发布的镜像：<a target="_blank" rel="noopener" href="https://hub.docker.com/r/pytorch/pytorch/tags">https://hub.docker.com/r/pytorch/pytorch/tags</a></p>
</li>
<li><p>NVIDIA为自己的GPU专门发布的PyTorch镜像：<a target="_blank" rel="noopener" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags">https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags</a></p>
</li>
</ul>
<p>所使用的镜像如下表所示：</p>
<table>
<thead>
<tr>
<th>PyTorch版本</th>
<th>PyTorch官方镜像</th>
<th>NVIDIA镜像</th>
</tr>
</thead>
<tbody><tr>
<td>PyTorch 1.6</td>
<td>pytorch&#x2F;pytorch:1.6.0-cuda10.1-cudnn7-runtime</td>
<td>nvcr.io&#x2F;nvidia&#x2F;pytorch:20.07-py3</td>
</tr>
<tr>
<td>PyTorch 1.7</td>
<td>pytorch&#x2F;pytorch:1.7.0-cuda11.0-cudnn8-runtime</td>
<td>nvcr.io&#x2F;nvidia&#x2F;pytorch:20.10-py3</td>
</tr>
<tr>
<td>PyTorch 1.8</td>
<td>pytorch&#x2F;pytorch:1.8.0-cuda11.1-cudnn8-runtime</td>
<td>nvcr.io&#x2F;nvidia&#x2F;pytorch:21.02-py3</td>
</tr>
<tr>
<td>PyTorch 1.9</td>
<td>pytorch&#x2F;pytorch:1.9.0-cuda11.1-cudnn8-runtime</td>
<td>nvcr.io&#x2F;nvidia&#x2F;pytorch:21.06-py3</td>
</tr>
<tr>
<td>PyTorch 1.10</td>
<td>pytorch&#x2F;pytorch:1.10.0-cuda11.3-cudnn8-runtime</td>
<td>nvcr.io&#x2F;nvidia&#x2F;pytorch:21.10-py3</td>
</tr>
<tr>
<td>PyTorch 1.11</td>
<td></td>
<td>nvcr.io&#x2F;nvidia&#x2F;pytorch:22.01-py3</td>
</tr>
</tbody></table>
<h3 id="1-2-测试方式"><a href="#1-2-测试方式" class="headerlink" title="1.2 测试方式"></a>1.2 测试方式</h3><p>直接按下述命令执行，然后使用命令 <code>nvidia-smi</code> 查看当前进程所占用的显存大小（此处以镜像包<code>nvcr.io/nvidia/pytorch:20.07-py3</code>为例）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mycomputer@mycomputer:~$ docker run -it --runtime=nvidia nvcr.io/nvidia/pytorch:20.07-py3 /bin/bash</span><br><span class="line">root@5b7fb9dd02fa:/workspace# python3</span><br><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; x = torch.tensor([1.0]).cuda()</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>

<h3 id="1-3-测试结果"><a href="#1-3-测试结果" class="headerlink" title="1.3 测试结果"></a>1.3 测试结果</h3><table>
<thead>
<tr>
<th></th>
<th align="center">NVIDIA镜像</th>
<th align="center">PyTorch官方镜像</th>
</tr>
</thead>
<tbody><tr>
<td>PyTorch 1.6</td>
<td align="center">591M</td>
<td align="center">783M</td>
</tr>
<tr>
<td>PyTorch 1.7</td>
<td align="center">625M</td>
<td align="center">825M</td>
</tr>
<tr>
<td>PyTorch 1.8</td>
<td align="center">693M</td>
<td align="center">941M</td>
</tr>
<tr>
<td>PyTorch 1.9</td>
<td align="center">761M</td>
<td align="center">1045M</td>
</tr>
<tr>
<td>PyTorch 1.10</td>
<td align="center">741M</td>
<td align="center">1113M</td>
</tr>
<tr>
<td>PyTorch 1.11</td>
<td align="center">741M</td>
<td align="center">-</td>
</tr>
</tbody></table>
<p>可以看出随着版本越高，PyTorch Context占用的显存越大，这个符合一般的理解。随着版本升高，一般都是往里加新功能，很少会减去功能的。</p>
<p>这里不太确定的是NVIDIA发布的镜像的PyTorch Context明显比PyTorch官方发布的镜像要小。现在还不确定是真实的就是小一些，还是上述测试方法有问题。</p>
<h3 id="1-4-作用"><a href="#1-4-作用" class="headerlink" title="1.4 作用"></a>1.4 作用</h3><p>在有些场景下不同镜像包之间的这几百M的差异并不重要，但是在有些场景下就有些重要了。比如仅做推理的线上服务，所有的算法服务都要同时部署，但是这些服务并不是每时每刻都在进行推理，所以此时GPU的计算资源是有空闲的，而GPU的显存却是紧缺的。如果能够降低PyTorch运行时上下文占用的显存就可以多部署一些服务。</p>
<h2 id="二、训练过程显存分析"><a href="#二、训练过程显存分析" class="headerlink" title="二、训练过程显存分析"></a>二、训练过程显存分析</h2><p>在该部分显存分析时使用函数 <code>torch.cuda.memory_allocated()</code> 获取当前所有张量的显存大小，该函数的返回值的单位是 <code>字节</code>。注意该函数获取到的显存大小与 <code>nvidia-smi</code> 获取到的显存大小是不同的，它们之间的差别在最后一部分说明。在该部分的分析过程中都使用函数 <code>torch.cuda.memory_allocated()</code> 获取的结果。</p>
<p>在这里为了简化，使用单层全连接作为模型。</p>
<h3 id="2-1-模型参数占显存大小"><a href="#2-1-模型参数占显存大小" class="headerlink" title="2.1 模型参数占显存大小"></a>2.1 模型参数占显存大小</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为: 8388608</span></span><br></pre></td></tr></table></figure>

<p>此时的参数量为 2M（2048*1024），显存大小为 8M（8388608），符合常规认识的每个参数用4个字节存储。</p>
<h3 id="2-2-输入占显存大小"><a href="#2-2-输入占显存大小" class="headerlink" title="2.2 输入占显存大小"></a>2.2 输入占显存大小</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为: 8192</span></span><br></pre></td></tr></table></figure>

<p>此时参数量为 2k（2048），显存大小为 8k（8192），也没有问题。</p>
<h3 id="2-3-前向传播占显存大小"><a href="#2-3-前向传播占显存大小" class="headerlink" title="2.3 前向传播占显存大小"></a>2.3 前向传播占显存大小</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">outputs = model(inputs)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为: 4096</span></span><br></pre></td></tr></table></figure>

<p><code>outpus</code>的参数量为 1k（1024），显存大小为 4k（4096）。</p>
<h3 id="2-4-反向传播计算梯度占显存大小"><a href="#2-4-反向传播计算梯度占显存大小" class="headerlink" title="2.4 反向传播计算梯度占显存大小"></a>2.4 反向传播计算梯度占显存大小</h3><h4 id="2-4-1-反向传播简述"><a href="#2-4-1-反向传播简述" class="headerlink" title="2.4.1 反向传播简述"></a>2.4.1 反向传播简述</h4><p>链式求导公式为：</p>
<p>$$ \frac{\partial E}{\partial w} &#x3D; \frac{\partial E}{\partial y} \frac{\partial y}{\partial w} $$</p>
<p>这里的 $\frac{\partial E}{\partial y}$ 是传入导Linear层的梯度，$\frac{\partial y}{\partial w}$ 是求解Linear层的梯度。</p>
<p>忽略 <code>bias</code> 后Linear层前向传播时的公式为 $y &#x3D; w*x $，求导后为：$\frac{\partial y}{\partial w} &#x3D; x$；</p>
<p>参照下面简化的Linear层的代码（该代码删掉了大量判断条件、输入参数、返回值、bias等，仅作为示意）来看，前向传播<code>forward()</code>函数为输入乘以权重矩阵，反向传播<code>backward()</code>为输入的梯度乘以本层求导后的结果$x$（即<code>input</code>）；</p>
<p>函数<code>backward()</code>计算出的梯度<code>grad_weight</code>维度与该Linear层的权重参数相同，该值是需要消耗显存进行存储的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, weight</span>):</span><br><span class="line">        self.save_for_backward(<span class="built_in">input</span>, weight)  <span class="comment"># 将输入和权重存储给反向传播时使用</span></span><br><span class="line">        output = <span class="built_in">input</span>.mm(weight.t())</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grad_output</span>):</span><br><span class="line">        <span class="built_in">input</span>, weight = self.saved_tensors  <span class="comment"># 就是第4行代码中存储的输入和权重</span></span><br><span class="line"></span><br><span class="line">        grad_weight = grad_output.t().mm(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> grad_weight</span><br></pre></td></tr></table></figure>

<p>关于PyTorch中反向传播更详细的内容可以参考官方文档 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/1.10/notes/autograd.html">自动求导机制</a> 和 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/1.10/notes/extending.html">扩展PyTorch</a>。</p>
<h4 id="2-4-2-反向传播显存"><a href="#2-4-2-反向传播显存" class="headerlink" title="2.4.2 反向传播显存"></a>2.4.2 反向传播显存</h4><p>存储计算出的loss值本身也需要少量的显存空间，先看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line">outputs = model(inputs)</span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">loss = torch.mean(outputs)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为: 512</span></span><br></pre></td></tr></table></figure>

<p>然后看反向传播计算梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line">outputs = model(inputs)</span><br><span class="line">loss = torch.mean(outputs)</span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为: 8388608</span></span><br></pre></td></tr></table></figure>

<p>梯度的参数量等同于权重$w$，为 2M（2048 * 1024），消耗的显存为 8M（8388608）；</p>
<h3 id="2-5-参数更新时的优化器状态占显存大小"><a href="#2-5-参数更新时的优化器状态占显存大小" class="headerlink" title="2.5 参数更新时的优化器状态占显存大小"></a>2.5 参数更新时的优化器状态占显存大小</h3><h4 id="2-5-1-SGD优化简述"><a href="#2-5-1-SGD优化简述" class="headerlink" title="2.5.1 SGD优化简述"></a>2.5.1 SGD优化简述</h4><p>不带动量的SGD优化公式为：</p>
<p>$$ \theta &#x3D; \theta - \eta \nabla_{\theta} J(\theta) $$</p>
<p>带动量的SGD的优化公式为：</p>
<p>$$ v_t &#x3D; \gamma v_{t-1} + \eta \nabla_{\theta} J(\theta) $$</p>
<p>$$ \theta &#x3D; \theta - v_t$$</p>
<p>简化后的SGD代码如下（<a target="_blank" rel="noopener" href="https://github.com/PyTorch/PyTorch/blob/v1.11.0-rc3/torch/optim/sgd.py">源码链接</a>），该代码删掉了大量判断条件、输入参数、返回值等，仅作为示意。代码中的<code>lr</code>为学习率，对应公式中的$\eta$；代码中的<code>momentum</code>为动量，对应公式中的$\gamma$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SGD</span>(<span class="title class_ inherited__">Optimizer</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params, defaults</span>):</span><br><span class="line">        self.state = defaultdict(<span class="built_in">dict</span>)  <span class="comment"># 用来缓存优化器状态</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self, closure=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            momentum = group[<span class="string">&quot;momentum&quot;</span>]  <span class="comment"># 动量</span></span><br><span class="line">            lr = group[<span class="string">&quot;lr&quot;</span>]  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># 如果当前参数梯度为None，则不需要更新</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                d_p = p.grad  <span class="comment"># 梯度</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> momentum != <span class="number">0</span>:  <span class="comment"># 如果动量不为0，则根据动量重新计算梯度</span></span><br><span class="line">                    <span class="comment"># 由下面这段代码可看出只有第一次执行时需要开辟新缓存，之后都是在更新self.state中的值</span></span><br><span class="line">                    state = self.state[p]</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&quot;momentum_buffer&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">                        buf = torch.clone(d_p).detach()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        buf = state[<span class="string">&quot;momentum_buffer&quot;</span>]</span><br><span class="line">                        buf.mul_(momentum).add_(d_p)</span><br><span class="line">                    state[<span class="string">&quot;momentum_buffer&quot;</span>] = buf</span><br><span class="line">                    d_p = d_p.add(buf, alpha=momentum)</span><br><span class="line"></span><br><span class="line">                p.add_(d_p, alpha=-lr)  <span class="comment"># 将（梯度 * -学习率）更新到权重参数上</span></span><br></pre></td></tr></table></figure>

<h4 id="2-5-2-SGD优化时的显存"><a href="#2-5-2-SGD优化时的显存" class="headerlink" title="2.5.2 SGD优化时的显存"></a>2.5.2 SGD优化时的显存</h4><p><strong>使用SGD作为优化器，不使用动量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line">outputs = model(inputs)</span><br><span class="line">loss = torch.mean(outputs)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为: 0</span></span><br></pre></td></tr></table></figure>

<p>由上面的SGD代码可知，当不使用动量时，优化器所做的工作其实就是一行代码 <code>p.add_(d_p, alpha=-lr)</code>，这里面只使用到了参数权重和其梯度，优化器本身没有任何需要存储的变量，所以新增的显存为0。</p>
<p><strong>使用SGD作为优化器，使用动量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line">outputs = model(inputs)</span><br><span class="line">loss = torch.mean(outputs)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为: 8388608</span></span><br></pre></td></tr></table></figure>

<p>由上面的SGD代码可知，当使用动量时，优化器需要在<code>self.state</code>中为每个参数存储一个<code>momentum_buffer</code>，每有一个参数，就需要一个对应的<code>momentum_buffer</code>，所以新增的量与参数量是相同的，新增的显存为 8M（8388608）。</p>
<p>这里只是以最简单的SGD优化器为例，优化器还有更多类型，不同的优化器根据其存储的中间量的不同，需要的显存也不尽相同，需要具体分析。</p>
<h3 id="2-6-训练过程总体分析"><a href="#2-6-训练过程总体分析" class="headerlink" title="2.6 训练过程总体分析"></a>2.6 训练过程总体分析</h3><p>上面把训练过程中的每个步骤都详细分析过了，下面总体看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># SGD使用动量</span></span><br><span class="line">inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line">outputs = model(inputs)</span><br><span class="line">loss = torch.mean(outputs)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：25178624</span></span><br></pre></td></tr></table></figure>

<p>这里的输出值<code>25178624字节</code>的显存由以下部分构成：</p>
<ul>
<li><code>8388608字节</code>: 8M，模型参数model消耗的显存；</li>
<li><code>8192字节</code>: 8k，输入inputs消耗的显存；</li>
<li><code>4096字节</code>: 4k，输出outputs消耗的显存；</li>
<li><code>512字节</code>: 512字节，存储loss消耗的显存；</li>
<li><code>8388608字节</code>: 8M，模型model中每个参数的梯度消耗的显存；</li>
<li><code>8388608字节</code>: 8M，优化器optimizer消耗的显存；</li>
</ul>
<p>另外，模拟一下实际训练时会训练很多个batch，代码如下，其输出结果和之前是相同的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">current_memory = torch.cuda.memory_allocated()  <span class="comment"># 记录当前使用的显存</span></span><br><span class="line">model = torch.nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># SGD使用动量</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    inputs = torch.tensor([<span class="number">1.0</span>] * <span class="number">2048</span>).cuda()</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = torch.mean(outputs)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated() - current_memory)  <span class="comment"># 打印增加的显存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：25178624</span></span><br></pre></td></tr></table></figure>

<h3 id="2-7-总结"><a href="#2-7-总结" class="headerlink" title="2.7 总结"></a>2.7 总结</h3><p>这里以一个最简单的Linear作为模型，分析了在整个训练过程中，各个环节所使用的显存大小。后面还可以对各个常见的模型结构CNN、LSTM、Transformer等进行分析，其整个分析思路和上面是相同的，所以只要完全清楚这些模型的细节，显存分析只是一个体力活。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><p>深度学习中GPU和显存分析: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31558973">https://zhuanlan.zhihu.com/p/31558973</a></p>
</li>
<li><p>PyTorch显存机制分析: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/424512257">https://zhuanlan.zhihu.com/p/424512257</a></p>
</li>
<li><p>PyTorch: <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</a></p>
</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>PyTorch训练显存分析</p><p><a href="http://mingchao.wang/daka/2022/05/09/NLP工程/PyTorch/性能优化/Pytorch训练显存分析/">http://mingchao.wang/daka/2022/05/09/NLP工程/PyTorch/性能优化/Pytorch训练显存分析/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>mingchao.wang</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2022-05-09</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-05-09</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/daka/tags/PyTorch/">PyTorch</a><a class="link-muted mr-2" rel="tag" href="/daka/tags/%E6%98%BE%E5%AD%98/">显存</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/daka/2022/05/09/NLP%E7%AE%97%E6%B3%95/PyTorch%E4%B8%AD%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%BA%90%E7%A0%81%E5%92%8C%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">PyTorch中优化器的源码和显存分析</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/daka/2022/05/09/NLP%E5%B7%A5%E7%A8%8B/PyTorch/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/PyTorch%E4%B8%AD%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%BA%90%E7%A0%81%E5%92%8C%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/"><span class="level-item">PyTorch中优化器的源码和显存分析</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#一、PyTorch-Context"><span class="level-left"><span class="level-item">1</span><span class="level-item">一、PyTorch Context</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-测试环境"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">1.1 测试环境</span></span></a></li><li><a class="level is-mobile" href="#1-2-测试方式"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">1.2 测试方式</span></span></a></li><li><a class="level is-mobile" href="#1-3-测试结果"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">1.3 测试结果</span></span></a></li><li><a class="level is-mobile" href="#1-4-作用"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">1.4 作用</span></span></a></li></ul></li><li><a class="level is-mobile" href="#二、训练过程显存分析"><span class="level-left"><span class="level-item">2</span><span class="level-item">二、训练过程显存分析</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-模型参数占显存大小"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">2.1 模型参数占显存大小</span></span></a></li><li><a class="level is-mobile" href="#2-2-输入占显存大小"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">2.2 输入占显存大小</span></span></a></li><li><a class="level is-mobile" href="#2-3-前向传播占显存大小"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">2.3 前向传播占显存大小</span></span></a></li><li><a class="level is-mobile" href="#2-4-反向传播计算梯度占显存大小"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">2.4 反向传播计算梯度占显存大小</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-4-1-反向传播简述"><span class="level-left"><span class="level-item">2.4.1</span><span class="level-item">2.4.1 反向传播简述</span></span></a></li><li><a class="level is-mobile" href="#2-4-2-反向传播显存"><span class="level-left"><span class="level-item">2.4.2</span><span class="level-item">2.4.2 反向传播显存</span></span></a></li></ul></li><li><a class="level is-mobile" href="#2-5-参数更新时的优化器状态占显存大小"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">2.5 参数更新时的优化器状态占显存大小</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-5-1-SGD优化简述"><span class="level-left"><span class="level-item">2.5.1</span><span class="level-item">2.5.1 SGD优化简述</span></span></a></li><li><a class="level is-mobile" href="#2-5-2-SGD优化时的显存"><span class="level-left"><span class="level-item">2.5.2</span><span class="level-item">2.5.2 SGD优化时的显存</span></span></a></li></ul></li><li><a class="level is-mobile" href="#2-6-训练过程总体分析"><span class="level-left"><span class="level-item">2.6</span><span class="level-item">2.6 训练过程总体分析</span></span></a></li><li><a class="level is-mobile" href="#2-7-总结"><span class="level-left"><span class="level-item">2.7</span><span class="level-item">2.7 总结</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">3</span><span class="level-item">Reference</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/daka/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/daka/categories/NLP%E5%B7%A5%E7%A8%8B/"><span class="level-start"><span class="level-item">NLP工程</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/daka/categories/NLP%E5%B7%A5%E7%A8%8B/PyTorch/"><span class="level-start"><span class="level-item">PyTorch</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/daka/categories/NLP%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">NLP算法</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/daka/categories/NLP%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"><span class="level-start"><span class="level-item">文本向量表示</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/daka/categories/%E5%85%B6%E4%BB%96/"><span class="level-start"><span class="level-item">其他</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/daka/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/"><span class="level-start"><span class="level-item">测试类文章</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/daka/categories/%E7%88%B6%E7%B1%BB-%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"><span class="level-start"><span class="level-item">父类-测试文章</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/daka/categories/%E7%88%B6%E7%B1%BB-%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/%E5%AD%90%E7%B1%BB-%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"><span class="level-start"><span class="level-item">子类-测试文章</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/daka/tags/MacOS/"><span class="tag">MacOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/daka/tags/PyTorch/"><span class="tag">PyTorch</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/daka/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"><span class="tag">优化器</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/daka/tags/%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA/"><span class="tag">向量表示</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/daka/tags/%E6%98%BE%E5%AD%98/"><span class="tag">显存</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/daka/tags/%E6%B5%8B%E8%AF%95/"><span class="tag">测试</span><span class="tag">10</span></a></div></div></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.906Z">2022-05-09</time></p><p class="title"><a href="/daka/2022/05/09/test/hello-world-8/">Hello World-8</a></p><p class="categories"><a href="/daka/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.906Z">2022-05-09</time></p><p class="title"><a href="/daka/2022/05/09/test/hello-world-9/">Hello World-9</a></p><p class="categories"><a href="/daka/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.905Z">2022-05-09</time></p><p class="title"><a href="/daka/2022/05/09/test/hello-world-7/">Hello World-8</a></p><p class="categories"><a href="/daka/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.905Z">2022-05-09</time></p><p class="title"><a href="/daka/2022/05/09/test/hello-world-6/">Hello World-6</a></p><p class="categories"><a href="/daka/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-08T16:00:17.904Z">2022-05-09</time></p><p class="title"><a href="/daka/2022/05/09/test/hello-world-3/">Hello World-3</a></p><p class="categories"><a href="/daka/categories/%E6%B5%8B%E8%AF%95%E7%B1%BB%E6%96%87%E7%AB%A0/">测试类文章</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/daka/"><img src="/daka/img/logo.svg" alt="wmc1992" height="28"></a><p class="is-size-7"><span>&copy; 2022 mingchao.wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/wmc1992/daka"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/daka/js/column.js"></script><script src="/daka/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/daka/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/daka/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/daka/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/daka/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>